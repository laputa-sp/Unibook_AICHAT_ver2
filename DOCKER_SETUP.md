# Docker í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

ì´ í”„ë¡œì íŠ¸ëŠ” **Docker í™˜ê²½ì—ì„œ ì‹¤í–‰**ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ë‹¤ìŒ Docker ì»¨í…Œì´ë„ˆë“¤ì´ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤:
1. **vLLM** - LLM ì¶”ë¡  ì„œë²„
2. **Qdrant** - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

## ğŸ³ Docker ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

### ì˜µì…˜ 1: ë™ì¼ Docker ë„¤íŠ¸ì›Œí¬ (ê¶Œì¥)

ëª¨ë“  ì»¨í…Œì´ë„ˆë¥¼ ë™ì¼í•œ Docker ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°:

```bash
# Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create unibook-network

# vLLM ì»¨í…Œì´ë„ˆ (ë„¤íŠ¸ì›Œí¬ ì—°ê²°)
docker run -d --gpus all \
  --network unibook-network \
  --name vllm_gpt \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model openai/gpt-oss-20b \
  --max-model-len 8192

# Qdrant ì»¨í…Œì´ë„ˆ (ë„¤íŠ¸ì›Œí¬ ì—°ê²°)
docker run -d \
  --network unibook-network \
  --name qdrant \
  -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# Python ì•±ë„ ë™ì¼ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‹¤í–‰
docker run -d \
  --network unibook-network \
  --name unibook_api \
  -p 7861:7861 \
  -v $(pwd):/app \
  -w /app \
  python:3.9 \
  bash -c "pip install -r requirements.txt && python run.py"
```

**.env ì„¤ì •**:
```bash
VLLM_BASE_URL=http://vllm_gpt:8000    # ì»¨í…Œì´ë„ˆ ì´ë¦„ ì‚¬ìš©
QDRANT_HOST=qdrant                     # ì»¨í…Œì´ë„ˆ ì´ë¦„ ì‚¬ìš©
QDRANT_PORT=6333
```

### ì˜µì…˜ 2: í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬

Python ì•±ì„ í˜¸ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê³  Docker ì„œë¹„ìŠ¤ì— ì ‘ê·¼í•˜ëŠ” ê²½ìš°:

```bash
# vLLM ì»¨í…Œì´ë„ˆ
docker run -d --gpus all \
  --name vllm_gpt \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:latest \
  --model openai/gpt-oss-20b \
  --max-model-len 8192

# Qdrant ì»¨í…Œì´ë„ˆ
docker run -d \
  --name qdrant \
  -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# Python ì•±ì€ í˜¸ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰
python run.py
```

**.env ì„¤ì •**:
```bash
VLLM_BASE_URL=http://localhost:8000   # localhost ì‚¬ìš©
QDRANT_HOST=localhost                  # localhost ì‚¬ìš©
QDRANT_PORT=6333
```

## ğŸ” ì—°ê²° í™•ì¸

### vLLM ìƒíƒœ í™•ì¸
```bash
curl http://localhost:8000/v1/models
# ë˜ëŠ” Docker ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ
curl http://vllm_gpt:8000/v1/models
```

### Qdrant ìƒíƒœ í™•ì¸
```bash
curl http://localhost:6333/collections
# ë˜ëŠ” Docker ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ
curl http://qdrant:6333/collections
```

### ì•± Health Check
```bash
curl http://localhost:7861/health
```

## ğŸ› ï¸ í¬íŠ¸ ì¶©ëŒ í•´ê²°

ê¸°ë³¸ í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°:

```bash
# vLLMì„ 8001 í¬íŠ¸ë¡œ ì‹¤í–‰
docker run -d --gpus all \
  --name vllm_gpt \
  -p 8001:8000 \
  ...

# Qdrantë¥¼ 6334 í¬íŠ¸ë¡œ ì‹¤í–‰
docker run -d \
  --name qdrant \
  -p 6334:6333 \
  ...
```

**.env ìˆ˜ì •**:
```bash
VLLM_BASE_URL=http://localhost:8001
QDRANT_PORT=6334
```

## ğŸ› ë¬¸ì œ í•´ê²°

### "Connection refused" ì—ëŸ¬

**ì›ì¸**: Docker ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì˜¤ë¥˜

**í•´ê²°**:
```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps -a | grep -E "vllm|qdrant"

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart vllm_gpt qdrant

# ë¡œê·¸ í™•ì¸
docker logs vllm_gpt
docker logs qdrant
```

### "Cannot find container" ì—ëŸ¬

**ì›ì¸**: `.env`ì˜ í˜¸ìŠ¤íŠ¸ëª…ì´ Docker ë„¤íŠ¸ì›Œí¬ì— ë§ì§€ ì•ŠìŒ

**í•´ê²°**:
- Docker ë„¤íŠ¸ì›Œí¬ ë‚´: ì»¨í…Œì´ë„ˆ ì´ë¦„ ì‚¬ìš© (`vllm_gpt`, `qdrant`)
- í˜¸ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰: `localhost` ì‚¬ìš©

### GPU ê´€ë ¨ ì—ëŸ¬

**ì›ì¸**: NVIDIA Container Toolkit ë¯¸ì„¤ì¹˜

**í•´ê²°**:
```bash
# NVIDIA Container Toolkit ì„¤ì¹˜ í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# ì„¤ì¹˜ ë°©ë²•: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

## ğŸ“š ì°¸ê³ 

- ì´ í”„ë¡œì íŠ¸ëŠ” ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½(`localhost`)ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.
- ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ë³¸ì¸ì˜ Docker í™˜ê²½ì— ë§ê²Œ ì„¤ì •ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
- Docker Composeë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì¶”í›„ ì¶”ê°€ ì˜ˆì •)
