"""
Embedding Service - BGE-M3 ê¸°ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
BGE-M3ëŠ” ë‹¤êµ­ì–´ ì§€ì›ì´ ìš°ìˆ˜í•œ BAAIì˜ ì„ë² ë”© ëª¨ë¸
"""
import logging
from typing import List, Union
import numpy as np
from FlagEmbedding import BGEM3FlagModel

logger = logging.getLogger(__name__)


class EmbeddingService:
    """
    BGE-M3 ê¸°ë°˜ ì„ë² ë”© ì„œë¹„ìŠ¤

    BGE-M3 íŠ¹ì§•:
    - ë‹¤êµ­ì–´ ì§€ì› (100+ languages)
    - ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì› (ìµœëŒ€ 8192 tokens)
    - Dense + Sparse + ColBERT í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›
    - í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜
    """

    def __init__(self, model_name: str = "BAAI/bge-m3", use_fp16: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            model_name: ëª¨ë¸ëª… (ê¸°ë³¸: BAAI/bge-m3)
            use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€ (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
        """
        self.model_name = model_name
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.use_fp16 = use_fp16
        self.embedding_dim = 1024  # BGE-M3ì˜ ì„ë² ë”© ì°¨ì›

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (lazy loading)"""
        if self.model is None:
            self.logger.info(f"ğŸ“¥ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... ({self.model_name})")
            try:
                self.model = BGEM3FlagModel(
                    self.model_name,
                    use_fp16=self.use_fp16
                )
                self.logger.info("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

    def embed_text(self, text: str, max_length: int = 512) -> np.ndarray:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ í† í° ê¸¸ì´ (ê¸°ë³¸: 512, ìµœëŒ€: 8192)

        Returns:
            numpy array (1024 dim)
        """
        self.load_model()

        if not text or not text.strip():
            self.logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥, ì œë¡œ ë²¡í„° ë°˜í™˜")
            return np.zeros(self.embedding_dim)

        try:
            # BGE-M3 encode
            # return_dense=True: Dense ë²¡í„°ë§Œ ë°˜í™˜ (ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ìš©)
            # return_sparse=False, return_colbert_vecs=False: Sparse/ColBERT ë¯¸ì‚¬ìš©
            embeddings = self.model.encode(
                [text],
                batch_size=1,
                max_length=max_length,
                return_dense=True,
                return_sparse=False,
                return_colbert_vecs=False
            )

            # Dense ì„ë² ë”© ì¶”ì¶œ
            embedding_vector = embeddings['dense_vecs'][0]

            # numpy arrayë¡œ ë³€í™˜ (ì´ë¯¸ numpyì¼ ìˆ˜ ìˆìŒ)
            if not isinstance(embedding_vector, np.ndarray):
                embedding_vector = np.array(embedding_vector)

            return embedding_vector

        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ì œë¡œ ë²¡í„° ë°˜í™˜
            return np.zeros(self.embedding_dim)

    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        max_length: int = 512,
        show_progress: bool = True
    ) -> List[np.ndarray]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (íš¨ìœ¨ì )

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32)
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€

        Returns:
            numpy array ë¦¬ìŠ¤íŠ¸ (ê° 1024 dim)
        """
        self.load_model()

        if not texts:
            return []

        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        processed_texts = []
        indices_map = []  # ì›ë³¸ ì¸ë±ìŠ¤ ë§¤í•‘

        for i, text in enumerate(texts):
            if text and text.strip():
                processed_texts.append(text)
                indices_map.append(i)

        if not processed_texts:
            self.logger.warning("ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ, ì œë¡œ ë²¡í„° ë°˜í™˜")
            return [np.zeros(self.embedding_dim) for _ in texts]

        try:
            self.logger.info(f"ğŸ“Š ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(processed_texts)}ê°œ)")

            # BGE-M3 ë°°ì¹˜ ì¸ì½”ë”©
            # Note: show_progress_bar is not supported, use show_progress instead
            embeddings = self.model.encode(
                processed_texts,
                batch_size=batch_size,
                max_length=max_length,
                return_dense=True,
                return_sparse=False,
                return_colbert_vecs=False
            )

            # Dense ë²¡í„° ì¶”ì¶œ
            dense_vecs = embeddings['dense_vecs']

            # ì›ë³¸ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì¬êµ¬ì„± (ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œë¡œ ë²¡í„°)
            result = [np.zeros(self.embedding_dim) for _ in texts]
            for i, original_idx in enumerate(indices_map):
                result[original_idx] = dense_vecs[i]

            self.logger.info(f"âœ… ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ!")
            return result

        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ì œë¡œ ë²¡í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return [np.zeros(self.embedding_dim) for _ in texts]

    def get_query_embedding(self, query: str) -> np.ndarray:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© (ë³„ë„ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥)

        BGE-M3ëŠ” ì¿¼ë¦¬/ë¬¸ì„œ êµ¬ë¶„ì´ í•„ìš”ì—†ìœ¼ë‚˜,
        í–¥í›„ ëª¨ë¸ êµì²´ ì‹œë¥¼ ìœ„í•´ ë³„ë„ ë©”ì„œë“œ ì œê³µ

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬

        Returns:
            numpy array (1024 dim)
        """
        # BGE-M3ëŠ” ëŒ€ì¹­ì  ëª¨ë¸ì´ë¯€ë¡œ ì¼ë°˜ ì„ë² ë”©ê³¼ ë™ì¼
        return self.embed_text(query, max_length=512)

    def similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            vec1, vec2: ì„ë² ë”© ë²¡í„°

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (-1 ~ 1)
        """
        # ì œë¡œ ë²¡í„° ì²´í¬
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        return float(np.dot(vec1, vec2) / (norm1 * norm2))


# Singleton ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
_embedding_service = None

def get_embedding_service() -> EmbeddingService:
    """
    ì„ë² ë”© ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ë°˜í™˜

    Returns:
        EmbeddingService ì¸ìŠ¤í„´ìŠ¤
    """
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service
