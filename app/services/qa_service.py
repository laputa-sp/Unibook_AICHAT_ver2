"""
QA Service - Complete QA Flow
Ports the entire jsonResultsService.handleJsonResults() logic from Node.js
Maintains 100% compatibility with original QA flow
Enhanced with conversation context support
"""
import logging
import time
import json
import asyncio
import re
from typing import Dict, List, Optional, Any, Tuple

from app.services.llm_service import llm_service
from app.services.pdf_database import pdf_database
from app.services.qdrant_service import get_qdrant_service
from app.utils.conversation_cache import ConversationCache
from app.utils.llm_cache import get_llm_cache
from app.utils.conversation_logger import get_conversation_logger
from app.utils.query_logger import get_query_logger

logger = logging.getLogger(__name__)


class QAService:
    """
    Complete QA service that replicates Node.js jsonResultsService logic

    Flow:
    1. Get PDF metadata
    2. Get conversation history
    3. Call get_search_type() to determine search strategy
    4. Based on searchType, collect data (summary/toc/keyword/page)
    5. Call get_answer() with collected data
    6. Return results with metadata

    Enhanced with:
    - Conversation context support
    - LLM response caching
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        # Conversation cache for context persistence
        self.conversations = ConversationCache(max_size=1000, max_age_hours=24)
        # Context storage for followup questions
        self.context_store = {}
        # LLM response cache
        self.llm_cache = get_llm_cache()
        # Conversation file logger
        self.conversation_logger = get_conversation_logger()
        # Conversation summary storage (ì„¸ì…˜ë³„ ìš”ì•½ ê´€ë¦¬)
        # {session_id: {"summary": str, "recent_turns": List[Dict]}}
        self.conversation_summaries = {}
        # ìµœê·¼ ëŒ€í™” ìµœëŒ€ ê°œìˆ˜
        self.MAX_RECENT_TURNS = 5

    async def handle_json_results(
        self,
        isbn: str,
        query: str,
        page_from: Optional[int],
        selected_text: Optional[str],
        userid: str,
        sessionid: str
    ) -> Dict[str, Any]:
        """
        Main QA handler - replicates jsonResultsService.handleJsonResults()

        Args:
            isbn: Book ISBN
            query: User query
            page_from: Optional starting page number
            selected_text: Optional selected text by user
            userid: User ID
            sessionid: Session ID

        Returns:
            Dict with infos, chunks, visualElements, contextInfo
        """
        # ğŸ” Query Logger ì´ˆê¸°í™”
        query_logger = get_query_logger()
        log_id = query_logger.start_log(
            userid=userid,
            sessionid=sessionid,
            isbn=isbn,
            query=query,
            select_text=selected_text
        )
        logger.info(f"ğŸ“Š [QueryLog:{log_id}] Started tracking query: {query[:50]}")

        # Initialize variables for all code paths
        pages_spec = None
        pages_found = []
        search_type = None

        # 1) Get PDF metadata
        pdf = await pdf_database.get_pdf_by_isbn(isbn)
        if not pdf:
            raise ValueError(f"PDF with ISBN {isbn} not found")

        record = {
            'id': pdf['id'],
            'isbn': pdf['isbn'],
            'title': pdf.get('title', 'Unknown'),
            'file_path': pdf.get('file_path', '')
        }

        # ğŸš€ ì „ì²´ ìš”ì²­ ìºì‹œ ì²´í¬ (ë¹ ë¥¸ ê²½ë¡œ)
        # ë‹¨, ë¬¸ë§¥ ì˜ì¡´ì ì¸ ì§ˆë¬¸ì€ ìºì‹±í•˜ì§€ ì•ŠìŒ (ëŒ€í™” íë¦„ ë°˜ì˜ í•„ìš”)
        context_dependent_keywords = [
            # ìƒì„¸ë„ ê´€ë ¨
            'ë”', 'ìì„¸', 'ìƒì„¸', 'êµ¬ì²´',
            # ì—°ì†ì„± ê´€ë ¨
            'ê³„ì†', 'ì´ì–´', 'ì¶”ê°€', 'ë‹¤ì‹œ',
            # ì ‘ì†ì‚¬/ì§€ì‹œì–´
            'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ',
            # ì§€ì‹œëŒ€ëª…ì‚¬
            'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
            'ì´ì „', 'ë°©ê¸ˆ', 'ì•„ê¹Œ',
            # ì§§ì€ ëŒ€ë‹µ/ì§ˆë¬¸ (ê±°ì˜ í•­ìƒ ë¬¸ë§¥ ì˜ì¡´)
            'ë„¤', 'ì‘', 'ì™œ', 'ì˜ˆë¥¼', 'ì–´ë–»ê²Œ'
        ]

        # ë§¤ìš° ì§§ì€ ì§ˆë¬¸ (<= 3ì)ì€ ê±°ì˜ í•­ìƒ ë¬¸ë§¥ ì˜ì¡´
        is_very_short = len(query.strip()) <= 3

        is_context_dependent = any(keyword in query for keyword in context_dependent_keywords) or is_very_short

        # ğŸ“Š Step 2: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        conversation_context_formatted = await self._format_conversation_context(sessionid)
        previous_context = await self._get_latest_conversation_context(userid, sessionid)

        # ëŒ€í™” ê¸°ë¡ ê°œìˆ˜ ê³„ì‚°
        conv_history = await self.conversations.get(sessionid)
        conv_turns = len(conv_history) if conv_history else 0

        query_logger.log_conversation_context(
            has_history=(conv_turns > 0),
            turns=conv_turns,
            context_dependent=is_context_dependent
        )

        # ğŸ“Š Step 3: ìºì‹œ ì²´í¬
        cache_start = time.time()
        if not is_context_dependent:
            # ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ ìºì‹± (ì¿¼ë¦¬ëŠ” llm_cacheì—ì„œ ì •ê·œí™”ë¨)
            full_cache_context = f"full:{isbn}"
            cached_full_result = self.llm_cache.get(query, full_cache_context)
            cache_time_ms = (time.time() - cache_start) * 1000

            if cached_full_result:
                self.logger.info(f"âš¡ FULL CACHE HIT - Instant response: {query[:50]}")
                query_logger.log_cache_check('full', True, cache_time_ms)

                # ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ì‹œ ë¡œê·¸ ì €ì¥í•˜ê³  ë°˜í™˜
                await query_logger.save_log(
                    answer=cached_full_result.get('answer', ''),
                    chunks_count=len(cached_full_result.get('chunks', [])),
                    visual_elements_count=len(cached_full_result.get('visualElements', []))
                )
                return cached_full_result
            else:
                query_logger.log_cache_check('full', False, cache_time_ms)
        else:
            self.logger.info(f"ğŸ”„ Context-dependent query, skipping cache: {query[:50]}")
            query_logger.log_cache_check('none', False, 0)

        # ğŸ“Š Step 4: TOC ë¡œë“œ
        toc_start = time.time()
        toc_core_summary = await self._get_toc_core_summary_text(record['id'])
        toc = toc_core_summary  # Use core_summary for LLM (includes keywords like MBTI)
        toc_time_ms = (time.time() - toc_start) * 1000

        query_logger.log_toc_load(
            toc_type='core_summary',
            toc_length=len(toc),
            load_time_ms=toc_time_ms
        )

        # Prepare user search
        user_search = (query or '').strip()
        if not user_search:
            user_search = "ì•ˆë…•í•˜ì„¸ìš”."

        # Handle direct title queries (bypass LLM to avoid hallucination)
        title_keywords = ['ì œëª©', 'ì±… ì´ë¦„', 'ì±…ì´ë¦„', 'íƒ€ì´í‹€', 'title', 'ì´ ì±… ì´ë¦„', 'ë„ì„œëª…', 'ì„œëª…']
        query_lower = user_search.lower()
        if any(keyword in query_lower for keyword in title_keywords):
            # Check if this is a title-asking question (not just mentions title in passing)
            asking_patterns = ['ë­', 'ë¬´ì—‡', 'ì•Œë ¤', 'ì´ë¦„', '?', 'ë­ì•¼', 'ë­”ê°€', 'ë­¡ë‹ˆê¹Œ']
            if any(pattern in query_lower for pattern in asking_patterns):
                self.logger.info(f'ğŸ“š Direct title query detected: {user_search}')
                title_answer = f"ì´ ì±…ì˜ ì œëª©ì€ **{record['title']}** ì…ë‹ˆë‹¤."

                # Save to conversation cache
                await self.conversations.append(sessionid, [
                    {'role': 'user', 'content': user_search},
                    {'role': 'assistant', 'content': title_answer}
                ])

                # Return in the same format as normal responses
                infos = {
                    'userid': userid,
                    'sessionid': sessionid,
                    'ì œëª©': record['title'],
                    'isbn': record['isbn'],
                    'ìœ ì €ë°œí™”': user_search,
                    'ê²€ìƒ‰ìœ í˜•': 'title_query',
                    'ê²€ìƒ‰í‚¤ì›Œë“œ': None,
                    'í˜ì´ì§€ë²”ìœ„': None,
                    'ê²€ìƒ‰ëœí˜ì´ì§€': [],
                    'ê²€ìƒ‰ì„ íƒì´ìœ ': 'Direct title query detected',
                    'ê²€ìƒ‰ìš”ì²­LLMì‹œê°„': 0,
                    'ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰ì‹œê°„': 0,
                    'ë‹µë³€ìƒì„±ì‹œê°„': 0,
                    'answer': title_answer
                }

                return {
                    'infos': infos,
                    'chunks': [],
                    'visualElements': [],
                    'contextInfo': '{}'
                }

        # ğŸ“Š Step 5: LLM Call #1 - Determine search strategy
        # NOTE: irrelevant íŒë‹¨ì„ ìœ„í•´ ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì œê³µ (í›„ì† ì§ˆë¬¸ ê°ì§€)
        start_time = time.time()
        search_info_from_llm = await llm_service.get_search_type(
            title=record['title'],
            toc=toc,  # Use core_summary (includes keywords like MBTI)
            history_user_query=conversation_context_formatted,  # í›„ì† ì§ˆë¬¸ ê°ì§€ë¥¼ ìœ„í•´ í•„ìš”
            query=user_search,
            previous_context=previous_context
        )
        llm_time_ms = round((time.time() - start_time) * 1000, 1)

        search_result = search_info_from_llm['result']

        # ğŸ“Š Log LLM search type decision
        query_logger.log_llm_search_type(
            request_time_ms=llm_time_ms,
            search_type=search_result.get('searchType', 'unknown'),
            reason=search_result.get('reason', ''),
            core_keywords=search_result.get('coreKeywords', []),
            sub_keywords=search_result.get('subKeywords', []),
            page_range=search_result.get('pages'),
            use_previous_context=search_result.get('usePreviousContext', False)
        )

        # Handle previous context reuse for followup questions
        use_previous_context = search_result.get('usePreviousContext', False) and previous_context is not None
        if use_previous_context:
            self.logger.info(f'ğŸ”„ Using previous context for followup question')
            # Reuse search parameters from previous context
            search_result['searchType'] = previous_context.get('searchType', search_result.get('searchType'))
            search_result['pages'] = previous_context.get('pages', search_result.get('pages', ''))
            search_result['coreKeywords'] = previous_context.get('coreKeywords', search_result.get('coreKeywords', []))
            search_result['subKeywords'] = previous_context.get('subKeywords', search_result.get('subKeywords', []))

        # Extract page range from LLM result
        from_page = None
        to_page = None
        if 'startPage' in search_result:
            sp = search_result.get('startPage')
            ep = search_result.get('endPage')
            if sp and int(sp) > 0:
                from_page = int(sp)
            if ep and int(ep) > 0:
                to_page = int(ep)

            if from_page and not to_page:
                to_page = from_page
            if to_page and not from_page:
                from_page = to_page

        # 5) Collect data based on search type
        # NOTE: í•­ìƒ ìƒˆë¡œ ê²€ìƒ‰ (previous_contextì—ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ìˆìŒ)
        # usePreviousContext=true ì´ë©´ ì´ì „ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¡œ ì¬ê²€ìƒ‰
        book_texts = ""
        texts_summary = ""
        db_time_ms = 0
        pages_found = []
        search_logs = []
        chunks = []
        visual_elements = []

        core_keywords = search_result.get('coreKeywords', [])
        sub_keywords = search_result.get('subKeywords', [])
        search_type = search_result.get('searchType', 'keyword')
        pages_spec = search_result.get('pages', '')

        if use_previous_context:
            self.logger.info(f'ğŸ”„ Re-searching with previous metadata: {search_type}, pages={pages_spec}')

        # í•­ìƒ ê²€ìƒ‰ ìˆ˜í–‰ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
        if True:  # ê¸°ì¡´ ì¡°ê±´ë¬¸ ì œê±°
            if search_type == "irrelevant":
                # ì±…ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ - ê²€ìƒ‰ ìƒëµ
                book_texts = ""
                texts_summary = ""
                pages_found = []
                self.logger.info(f'âš ï¸ Irrelevant question detected: {user_search}')

            elif search_type == "summary":
                # Get summary data from TOC
                texts_summary = await self._get_toc_summaries_by_page_spec(record['id'], pages_spec)
                # Parse pages_spec to populate pages_found for source citation (supports multiple ranges like "580-582,628,687-688")
                pages_found = self._parse_page_spec(pages_spec) if pages_spec else []

            elif search_type == "toc":
                # TOC query - no content needed
                book_texts = ""
                pages_found = []

            else:
                # keyword or page search
                has_keywords = len(core_keywords) > 0

                if not has_keywords:
                    # No keywords - get pages directly
                    if not pages_spec:
                        pages_spec = f"{from_page}-{to_page}" if from_page else "1-5"

                    start_time = time.time()
                    chunks = await self._get_chunks_text_by_page_spec(record['id'], pages_spec)
                    db_time_ms = round((time.time() - start_time) * 1000, 3)

                    if chunks:
                        pages_found = sorted(set(c['page'] for c in chunks))

                        if len(pages_found) <= 20:
                            book_texts = "\n\n".join([
                                f"page: {c['page']}\n\në‚´ìš©: {c['content'].strip()}"
                                for c in chunks
                            ])
                        else:
                            book_texts = ""

                else:
                    # Keyword search with FTS
                    pages_search = self._parse_page_spec(pages_spec) if pages_spec else None

                    start_time = time.time()
                    result = await self._fetch_results_with_keywords(
                        pdf_id=record['id'],
                        core_keywords=core_keywords,
                        sub_keywords=sub_keywords,
                        pages=pages_search
                    )
                    db_time_ms = round((time.time() - start_time) * 1000, 3)

                    chunks = result['chunks']
                    visual_elements = result.get('visualElements', [])
                    search_logs = result.get('searchLogs', [])

                    # ğŸ“Š Log DB search (FTS5)
                    pages_found_list = sorted(set(c['page'] for c in chunks)) if chunks else []
                    query_logger.log_db_search(
                        method='fts5',
                        query=f"FTS5 MATCH with {len(core_keywords)} core + {len(sub_keywords)} sub keywords",
                        time_ms=db_time_ms,
                        results_count=len(chunks),
                        pages_found=pages_found_list,
                        extra_data={
                            'fts5': {
                                'keywords_used': {
                                    'core': [kw.get('keyword', '') for kw in core_keywords],
                                    'sub': [kw.get('keyword', '') for kw in sub_keywords]
                                },
                                'page_restriction': pages_spec,
                                'fallback_triggered': False
                            }
                        }
                    )

                    if chunks:
                        pages_found = sorted(set(c['page'] for c in chunks))

                        if len(pages_found) <= 20:
                            book_texts = "\n\n".join([
                                f"page: {c['page']}\n\në‚´ìš©: {c['content'].strip()}"
                                for c in chunks
                            ])
                        else:
                            book_texts = ""

        # 6) LLM Call #2: Get answer (with caching)
        answer = ""
        answer_token = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
        answer_time_ms = 0
        cache_hit = False

        if user_search:
            # Create cache context key (ISBN + search_typeë§Œ í¬í•¨, ë‚´ìš© ì œì™¸)
            # ê°™ì€ ì§ˆë¬¸ì´ë©´ í•­ìƒ ê°™ì€ ìºì‹œ í‚¤ ìƒì„± â†’ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ
            cache_context = f"{isbn}:{search_type}"

            # ë¬¸ë§¥ ì˜ì¡´ì ì¸ ì§ˆë¬¸ì€ LLM ë‹µë³€ ìºì‹œë„ ìŠ¤í‚µ
            context_dependent_keywords = [
                # ìƒì„¸ë„ ê´€ë ¨
                'ë”', 'ìì„¸', 'ìƒì„¸', 'êµ¬ì²´',
                # ì—°ì†ì„± ê´€ë ¨
                'ê³„ì†', 'ì´ì–´', 'ì¶”ê°€', 'ë‹¤ì‹œ',
                # ì ‘ì†ì‚¬/ì§€ì‹œì–´
                'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ',
                # ì§€ì‹œëŒ€ëª…ì‚¬
                'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤',
                'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
                'ì´ì „', 'ë°©ê¸ˆ', 'ì•„ê¹Œ',
                # ì§§ì€ ëŒ€ë‹µ/ì§ˆë¬¸
                'ë„¤', 'ì‘', 'ì™œ', 'ì˜ˆë¥¼', 'ì–´ë–»ê²Œ'
            ]
            is_very_short = len(user_search.strip()) <= 3
            is_context_dependent_query = any(keyword in user_search for keyword in context_dependent_keywords) or is_very_short

            # Check cache first (ë¬¸ë§¥ ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ)
            if not is_context_dependent_query:
                cached_response = self.llm_cache.get(user_search, cache_context)
            else:
                cached_response = None
                self.logger.info(f"ğŸ”„ Context-dependent, skip LLM cache: {user_search[:50]}")

            if cached_response:
                # Cache hit!
                cache_hit = True
                llm_answer = cached_response
                answer_time_ms = 0  # Instant from cache
                self.logger.info(f"âœ… LLM Cache HIT for: {user_search[:50]}")
            else:
                # Cache miss - call LLM
                start_time = time.time()

                if search_type == "summary":
                    llm_answer = await llm_service.get_answer(
                        title=record['title'],
                        texts=texts_summary,
                        history_user_query=conversation_context_formatted,
                        question=user_search,
                        toc=""
                    )
                elif search_type == "toc":
                    # Determine detail level based on query keywords
                    # Show full details if user asks for "ì „ì²´", "ì „ë¶€", "ëª¨ë‘", "ìƒì„¸", "ë‹¤ ë³´ì—¬"
                    query_lower = user_search.lower()
                    show_full_details = any(keyword in query_lower for keyword in ['ì „ì²´', 'ì „ë¶€', 'ëª¨ë‘', 'ìƒì„¸', 'ë‹¤ ë³´ì—¬', 'ì„¸ë¶€'])

                    # Always use full TOC data, let LLM filter based on prompt instruction
                    llm_answer = await llm_service.get_toc_answer(
                        title=record['title'],
                        toc=toc,  # Use core_summary (includes keywords)
                        question=user_search,
                        show_full_details=show_full_details,
                        history_user_query=conversation_context_formatted
                    )
                else:
                    llm_answer = await llm_service.get_answer(
                        title=record['title'],
                        texts=book_texts,
                        history_user_query=conversation_context_formatted,
                        question=user_search,
                        toc=toc  # Use core_summary (includes keywords)
                    )

                answer_time_ms = round((time.time() - start_time) * 1000, 1)

                # ğŸ“Š Log LLM answer generation
                query_logger.log_llm_answer(
                    request_time_ms=answer_time_ms,
                    prompt_length=len(book_texts) if book_texts else 0,
                    chunks_count=len(chunks) if chunks else 0,
                    answer_length=len(llm_answer.get('result', ''))
                )

                # Save to cache (ë¬¸ë§¥ ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ)
                if not is_context_dependent_query:
                    self.llm_cache.set(user_search, cache_context, llm_answer)
                    self.logger.info(f"ğŸ’¾ LLM Cache SET for: {user_search[:50]}")
                else:
                    self.logger.info(f"ğŸ”„ Context-dependent, LLM answer not cached: {user_search[:50]}")

            answer = llm_answer['result']
            answer_token = llm_answer['token']

        # ìë™ ì¶œì²˜ ìƒì„± (LLMì´ ì•„ë‹Œ ì‹œìŠ¤í…œì—ì„œ)
        source_citation = await self._generate_source_citation(
            pdf_id=record['id'],
            pages_found=pages_found,
            search_type=search_type
        )
        if source_citation:
            answer = answer + source_citation

        # Fetch visual elements (images, diagrams, tables) for found pages
        if pages_found:
            try:
                visual_elements = await pdf_database.get_visual_elements(
                    pdf_id=record['id'],
                    page_numbers=pages_found
                )
                self.logger.info(f"ğŸ“Š Found {len(visual_elements)} visual elements on pages {pages_found[:3]}...")
            except Exception as e:
                self.logger.error(f"âŒ Failed to fetch visual elements: {e}")
                visual_elements = []

        # 7) Build infos object (flat structure like Node.js)
        search_token = search_info_from_llm['token']
        core_keyword_str = ' '.join([f'"{k["keyword"]}"' for k in core_keywords]) if core_keywords else ""
        sub_keyword_str = ' '.join([f'"{k["keyword"]}"' for k in sub_keywords]) if sub_keywords else ""

        keyword_str = ""
        if core_keyword_str or sub_keyword_str:
            keyword_str = f"í•µì‹¬: {core_keyword_str}"
            if sub_keyword_str:
                keyword_str += f" / ë³´ì¡°: {sub_keyword_str}"

        infos = {
            'ë‹µë³€ì „ì²´_í† í°': f"input :{search_token.get('prompt_tokens', 0) + answer_token.get('prompt_tokens', 0)} output : {search_token.get('completion_tokens', 0) + answer_token.get('completion_tokens', 0)}",
            'userid': userid,
            'sessionid': sessionid,
            'ì œëª©': record['title'],
            'isbn': record['isbn'],
            'ì‚¬ìš©ìì •ë³´': "",
            'ìœ ì €ë°œí™”': query,
            'nowPage': "",
            'nowToc': "",
            'selectText': selected_text or None,

            'íƒ€ì…ê²€ìƒ‰ê²°ê³¼': str(search_info_from_llm),
            'ê²€ìƒ‰ìœ í˜•': search_result.get('searchType'),
            'ê²€ìƒ‰í‚¤ì›Œë“œ': keyword_str if keyword_str else None,
            'í˜ì´ì§€ë²”ìœ„': pages_spec or None,
            'ê²€ìƒ‰ëœí˜ì´ì§€': pages_found if pages_found else [],
            'ì‹œì‘í˜ì´ì§€': search_result.get('startPage'),
            'ëí˜ì´ì§€': search_result.get('endPage'),

            'ê²€ìƒ‰ì„ íƒì´ìœ ': search_result.get('reason'),
            'ê²€ìƒ‰ë¡œê·¸': str(search_logs) if search_logs else None,
            'ê²€ìƒ‰ìƒì„±ì‚¬ìš©í† í°ì •ë³´': search_token,
            'ê²€ìƒ‰ìƒì„±í† í°': search_token.get('prompt_tokens', 0),
            'ê²€ìƒ‰ìƒì„±ì•„ì›ƒí’‹í† í°': search_token.get('completion_tokens', 0),
            'ê²€ìƒ‰ìš”ì²­LLMì‹œê°„': llm_time_ms,

            'ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰ì‹œê°„': db_time_ms,

            'ë‹µë³€ìƒì„±ì‹œê°„': answer_time_ms,
            'ë‹µë³€ìƒì„±ì‚¬ìš©í† í°': answer_token,
            'ë‹µë³€ìƒì„±ì¸í’‹í† í°': answer_token.get('prompt_tokens', 0),
            'ë‹µë³€ìƒì„±ì•„ì›ƒí’‹í† í°': answer_token.get('completion_tokens', 0),
            'answer': answer or None,
        }

        # Context info for next turn
        # NOTE: texts/chunks ë“± ì›ë¬¸ ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í­ë°œ ë°©ì§€)
        # í•„ìš”ì‹œ ë©”íƒ€ë°ì´í„°ë¡œ ì¬ê²€ìƒ‰
        context_info = {
            'searchType': search_result.get('searchType'),
            'pages': pages_spec,
            'coreKeywords': core_keywords,
            'subKeywords': sub_keywords,
            'pagesFound': pages_found,
            'query': query  # ì§ì „ ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
            # 'texts': book_texts,  # âŒ ì œê±° - ì¬ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
            # 'texts_summary': texts_summary,  # âŒ ì œê±°
            # 'chunks': chunks,  # âŒ ì œê±°
            # 'visualElements': visual_elements  # âŒ ì œê±°
        }

        # Save context for followup questions
        context_key = f"{userid}:{sessionid}"
        self.context_store[context_key] = context_info
        self.logger.info(f'ğŸ’¾ Saved context (metadata only) for session: {sessionid}')

        # Save conversation history to cache
        await self.conversations.append(sessionid, [
            {'role': 'user', 'content': query},
            {'role': 'assistant', 'content': answer}
        ])
        self.logger.info(f'ğŸ’¬ Saved conversation to cache: {sessionid}')

        # Save conversation to file
        await self.conversation_logger.log_conversation(
            session_id=sessionid,
            user_id=userid,
            isbn=isbn,
            query=query,
            response=answer,
            metadata={
                'search_type': search_type,
                'pages': pages_spec,
                'pages_found': len(chunks) if chunks else 0,
                'response_length': len(answer)
            }
        )

        # Update conversation summary + recent turns (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ - ì‘ë‹µ ë¸”ë¡ ì•ˆ í•¨)
        asyncio.create_task(self._update_conversation_summary(
            sessionid=sessionid,
            user_query=query,
            assistant_response=answer,
            isbn=isbn
        ))

        # ğŸš€ ì „ì²´ ìš”ì²­ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥ (ë¬¸ë§¥ ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ)
        result = {
            'infos': infos,
            'chunks': chunks,
            'visualElements': visual_elements,
            'contextInfo': str(context_info)
        }

        # ë¬¸ë§¥ ì˜ì¡´ì ì¸ ì§ˆë¬¸ì€ ìºì‹±í•˜ì§€ ì•ŠìŒ
        context_dependent_keywords = [
            # ìƒì„¸ë„ ê´€ë ¨
            'ë”', 'ìì„¸', 'ìƒì„¸', 'êµ¬ì²´',
            # ì—°ì†ì„± ê´€ë ¨
            'ê³„ì†', 'ì´ì–´', 'ì¶”ê°€', 'ë‹¤ì‹œ',
            # ì ‘ì†ì‚¬/ì§€ì‹œì–´
            'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ',
            # ì§€ì‹œëŒ€ëª…ì‚¬
            'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
            'ì´ì „', 'ë°©ê¸ˆ', 'ì•„ê¹Œ',
            # ì§§ì€ ëŒ€ë‹µ/ì§ˆë¬¸
            'ë„¤', 'ì‘', 'ì™œ', 'ì˜ˆë¥¼', 'ì–´ë–»ê²Œ'
        ]
        is_very_short = len(query.strip()) <= 3
        is_context_dependent = any(keyword in query for keyword in context_dependent_keywords) or is_very_short

        if not is_context_dependent:
            full_cache_context = f"full:{isbn}"
            self.llm_cache.set(query, full_cache_context, result)
            self.logger.info(f"ğŸ’¾ FULL RESULT CACHED: {query[:50]}")
        else:
            self.logger.info(f"ğŸ”„ Context-dependent query, not cached: {query[:50]}")

        # ğŸ“Š Save query log to DB
        try:
            await query_logger.save_log(
                answer=answer or '',
                chunks_count=len(chunks),
                visual_elements_count=len(visual_elements)
            )
            self.logger.info(f"ğŸ“Š [QueryLog:{log_id}] Saved to DB")
        except Exception as e:
            self.logger.error(f"âŒ Failed to save query log: {e}")

        return result

    async def handle_json_results_stream(
        self,
        isbn: str,
        query: str,
        page_from: Optional[int],
        selected_text: Optional[str],
        userid: str,
        sessionid: str
    ):
        """
        Streaming version of handle_json_results

        Yields chunks of the answer in real-time as they're generated.
        Note: Streaming bypasses LLM cache for real-time delivery.

        Yields:
            Dict with:
            - chunk: Text chunk (or "" for metadata)
            - done: Boolean indicating completion
            - metadata: (Only on final message) Full response metadata
        """
        try:
            # â±ï¸ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()

            # ğŸ” Query Logger ì´ˆê¸°í™” (streaming)
            query_logger = get_query_logger()
            log_id = query_logger.start_log(
                userid=userid,
                sessionid=sessionid,
                isbn=isbn,
                query=query,
                select_text=selected_text
            )
            self.logger.info(f"ğŸ“Š [QueryLog:{log_id}] Started tracking query (streaming): {query[:50]}")

            # 1) Get PDF metadata
            pdf = await pdf_database.get_pdf_by_isbn(isbn)
            if not pdf:
                raise ValueError(f"PDF with ISBN {isbn} not found")

            record = {
                'id': pdf['id'],
                'isbn': pdf['isbn'],
                'title': pdf.get('title', 'Unknown'),
                'file_path': pdf.get('file_path', '')
            }

            # ğŸš€ ì „ì²´ ìš”ì²­ ìºì‹œ ì²´í¬ (ìŠ¤íŠ¸ë¦¬ë° APIë„ ìºì‹± ì§€ì›!)
            # ë¬¸ë§¥ ì˜ì¡´ì ì¸ ì§ˆë¬¸ì€ ìºì‹±í•˜ì§€ ì•ŠìŒ
            user_search = (query or '').strip()
            if not user_search:
                user_search = "ì•ˆë…•í•˜ì„¸ìš”."

            self.logger.info(f"ğŸ” [STREAM] Checking cache for: {user_search[:50]}")

            context_dependent_keywords = [
                'ë”', 'ìì„¸', 'ìƒì„¸', 'êµ¬ì²´',
                'ê³„ì†', 'ì´ì–´', 'ì¶”ê°€', 'ë‹¤ì‹œ',
                'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ',
                'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤',
                'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
                'ì´ì „', 'ë°©ê¸ˆ', 'ì•„ê¹Œ',
                'ë„¤', 'ì‘', 'ì™œ', 'ì˜ˆë¥¼', 'ì–´ë–»ê²Œ'
            ]

            is_very_short = len(user_search.strip()) <= 3
            is_context_dependent = any(keyword in user_search for keyword in context_dependent_keywords) or is_very_short

            if not is_context_dependent:
                # ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ ìºì‹±
                cache_start_time = time.time()
                full_cache_context = f"full:{isbn}"
                cached_full_result = self.llm_cache.get(user_search, full_cache_context)
                cache_hit_time_ms = round((time.time() - cache_start_time) * 1000, 3)

                # ğŸ“Š ë¡œê¹…: Cache Check
                query_logger.log_cache_check(
                    cache_type="full",
                    hit=cached_full_result is not None,
                    hit_time_ms=cache_hit_time_ms
                )

                if cached_full_result:
                    self.logger.info(f"âš¡ STREAM CACHE HIT - Instant response: {user_search[:50]}")

                    # ìºì‹œëœ ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ìŠ¤íŠ¸ë¦¬ë°
                    answer = cached_full_result['answer']
                    chunk_size = 10  # 10ìì”© ì²­í¬
                    for i in range(0, len(answer), chunk_size):
                        chunk = answer[i:i+chunk_size]
                        yield {
                            'chunk': chunk,
                            'done': False
                        }

                    # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì™„ë£Œ ì‹ í˜¸
                    yield {
                        'chunk': '',
                        'done': True,
                        'metadata': cached_full_result
                    }
                    return
                else:
                    self.logger.debug(f"ğŸ“Š [QueryLog] Cache miss: {user_search[:50]}")
            else:
                self.logger.info(f"ğŸ”„ Context-dependent query (stream), skipping cache: {user_search[:50]}")

            # 2) Get conversation context (summary + recent turns)
            # ê¸°ì¡´ ì „ì²´ íˆìŠ¤í† ë¦¬ ëŒ€ì‹  ìš”ì•½ + ìµœê·¼ Ní„´ë§Œ ì‚¬ìš© (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê³ ì •)
            conversation_context_formatted = await self._format_conversation_context(sessionid)

            previous_context = await self._get_latest_conversation_context(userid, sessionid)

            # ğŸ“Š ë¡œê¹…: Conversation Context
            conv_history = await self.conversations.get(sessionid)
            conv_turns = len(conv_history) if conv_history else 0
            query_logger.log_conversation_context(
                has_history=conv_turns > 0,
                turns=conv_turns,
                context_dependent=is_context_dependent
            )
            self.logger.debug(f"ğŸ“Š [QueryLog] Logged conversation context: {conv_turns} turns")

            # 2.1) Check if this is a new session - create session without greeting
            is_new_session = sessionid not in self.conversation_summaries or not self.conversation_summaries[sessionid]["recent_turns"]
            if is_new_session:
                self.logger.info(f"âœ¨ New session detected: {sessionid}")

                # Create session silently (no greeting message)
                await self.conversation_logger.create_session(
                    session_id=sessionid,
                    user_id=userid,
                    isbn=isbn,
                    system_message=""
                )

            # 3) Get TOC (Table of Contents) with core_summary for better keyword matching
            toc_start_time = time.time()
            toc_core_summary = await self._get_toc_core_summary_text(record['id'])
            toc = toc_core_summary  # Use core_summary for LLM (includes keywords like MBTI)
            toc_load_time_ms = round((time.time() - toc_start_time) * 1000, 3)

            # ğŸ“Š ë¡œê¹…: TOC Load
            query_logger.log_toc_load(
                toc_type="core_summary",
                toc_length=len(toc) if toc else 0,
                load_time_ms=toc_load_time_ms
            )
            self.logger.debug(f"ğŸ“Š [QueryLog] Loaded TOC: {len(toc) if toc else 0} chars in {toc_load_time_ms}ms")

            # user_searchëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì •ì˜ë¨ (ìºì‹œ ì²´í¬ ë¶€ë¶„)

            # Handle direct title queries (bypass LLM to avoid hallucination)
            title_keywords = ['ì œëª©', 'ì±… ì´ë¦„', 'ì±…ì´ë¦„', 'íƒ€ì´í‹€', 'title', 'ì´ ì±… ì´ë¦„', 'ë„ì„œëª…', 'ì„œëª…']
            query_lower = user_search.lower()
            if any(keyword in query_lower for keyword in title_keywords):
                # Check if this is a title-asking question (not just mentions title in passing)
                asking_patterns = ['ë­', 'ë¬´ì—‡', 'ì•Œë ¤', 'ì´ë¦„', '?', 'ë­ì•¼', 'ë­”ê°€', 'ë­¡ë‹ˆê¹Œ']
                if any(pattern in query_lower for pattern in asking_patterns):
                    self.logger.info(f'ğŸ“š Direct title query detected (streaming): {user_search}')
                    title_answer = f"ì´ ì±…ì˜ ì œëª©ì€ **{record['title']}** ì…ë‹ˆë‹¤."

                    # Save to conversation cache
                    await self.conversations.append(sessionid, [
                        {'role': 'user', 'content': user_search},
                        {'role': 'assistant', 'content': title_answer}
                    ])

                    # Return in the same format as normal streaming responses
                    infos = {
                        'userid': userid,
                        'sessionid': sessionid,
                        'ì œëª©': record['title'],
                        'isbn': record['isbn'],
                        'ìœ ì €ë°œí™”': user_search,
                        'ê²€ìƒ‰ìœ í˜•': 'title_query',
                        'ê²€ìƒ‰í‚¤ì›Œë“œ': None,
                        'í˜ì´ì§€ë²”ìœ„': None,
                        'ê²€ìƒ‰ëœí˜ì´ì§€': [],
                        'ê²€ìƒ‰ì„ íƒì´ìœ ': 'Direct title query detected',
                        'ê²€ìƒ‰ìš”ì²­LLMì‹œê°„': 0,
                        'ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰ì‹œê°„': 0,
                        'ë‹µë³€ìƒì„±ì‹œê°„': 0,
                        'answer': title_answer
                    }

                    # Yield the answer as a stream
                    yield {
                        'chunk': title_answer,
                        'done': False
                    }
                    yield {
                        'chunk': '',
                        'done': True,
                        'metadata': {
                            'infos': infos,
                            'chunks': [],
                            'visualElements': [],
                            'contextInfo': '{}'
                        }
                    }
                    return

            # 3.5) ğŸ”¥ Smart chapter detection for large TOCs
            # Pattern: "Nì¥ ìš”ì•½", "ì œNì¥", "Chapter N" etc.
            chapter_match = re.search(r'(?:ì œ?\s*)?(\d+)\s*ì¥', user_search)
            if chapter_match:
                chapter_num = chapter_match.group(1)
                self.logger.info(f"ğŸ” [Chapter Detection] Detected chapter {chapter_num} request")

                # Try to find chapter in TOC directly from database
                try:
                    import aiosqlite
                    async with aiosqlite.connect(pdf_database.db_path) as db:
                        # Get all entries that start with "Nì¥" (main chapter + all subsections)
                        # Match both "Nì¥ ..." and "N-1 ...", "N-2-1 ..." patterns
                        cursor = await db.execute(
                            """
                            SELECT title, start_page, end_page
                            FROM toc_entries
                            WHERE pdf_id = ? AND (title LIKE ? OR title LIKE ?)
                            ORDER BY start_page ASC
                            """,
                            (record['id'], f'{chapter_num}ì¥%', f'{chapter_num}-%')
                        )
                        rows = await cursor.fetchall()

                        if rows:
                            # Get main chapter title from first entry
                            chapter_title = rows[0][0]
                            # Get start page from first entry, end page from last entry
                            start_page = rows[0][1]
                            end_page = rows[-1][2]
                            pages_spec = f"{start_page}-{end_page}"

                            self.logger.info(f"âœ… [Chapter Detection] Found: {chapter_title} (pages {pages_spec})")
                            self.logger.info(f"   - Includes {len(rows)} sections from '{rows[0][0]}' to '{rows[-1][0]}'")

                            # Override search_result to use direct chapter lookup
                            search_result = {
                                'searchType': 'summary',
                                'pages': pages_spec,
                                'coreKeywords': [],
                                'subKeywords': [],
                                'usePreviousContext': False,
                                'reason': f'Chapter {chapter_num} detected via pattern matching ({len(rows)} sections, {start_page}-{end_page})'
                            }
                            llm_time_ms = 0.0  # Skip LLM call

                            # Skip LLM search type call
                            goto_search_result = True
                            chapter_detected_logged = True  # Flag to prevent duplicate logging
                        else:
                            self.logger.warning(f"âš ï¸ [Chapter Detection] Chapter {chapter_num} not found in TOC")
                            goto_search_result = False
                            chapter_detected_logged = False
                except Exception as e:
                    self.logger.error(f"âŒ [Chapter Detection] Error: {e}")
                    goto_search_result = False
                    chapter_detected_logged = False
            else:
                goto_search_result = False
                chapter_detected_logged = False

            if not goto_search_result:
                # 4) LLM Call #1: Determine search strategy (non-streaming)
                # NOTE: irrelevant íŒë‹¨ì„ ìœ„í•´ ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                llm_search_start_time = time.time()
                search_info_from_llm = await llm_service.get_search_type(
                    title=record['title'],
                    toc=toc,  # Use core_summary (includes keywords like MBTI)
                    history_user_query=conversation_context_formatted,  # í›„ì† ì§ˆë¬¸ ê°ì§€
                    query=user_search,
                    previous_context=previous_context
                )
                llm_time_ms = round((time.time() - llm_search_start_time) * 1000, 1)
                search_result = search_info_from_llm['result']
            else:
                # Skip LLM call, use detected chapter info
                search_info_from_llm = {'result': search_result, 'token': {}}

            # ğŸ“Š ë¡œê¹…: LLM Search Type Decision (if not already logged)
            if not chapter_detected_logged:
                query_logger.log_llm_search_type(
                    request_time_ms=llm_time_ms,
                    search_type=search_result.get('searchType', 'keyword'),
                    reason=search_result.get('reason', ''),
                core_keywords=search_result.get('coreKeywords', []),
                sub_keywords=search_result.get('subKeywords', []),
                page_range=search_result.get('pages', ''),
                use_previous_context=search_result.get('usePreviousContext', False)
            )
            self.logger.debug(f"ğŸ“Š [QueryLog] LLM decided: {search_result.get('searchType')} in {llm_time_ms}ms")

            # Handle previous context reuse
            use_previous_context = search_result.get('usePreviousContext', False) and previous_context is not None
            if use_previous_context:
                self.logger.info(f'ğŸ”„ Using previous context for followup question')
                search_result['searchType'] = previous_context.get('searchType', search_result.get('searchType'))
                search_result['pages'] = previous_context.get('pages', search_result.get('pages', ''))
                search_result['coreKeywords'] = previous_context.get('coreKeywords', search_result.get('coreKeywords', []))
                search_result['subKeywords'] = previous_context.get('subKeywords', search_result.get('subKeywords', []))

            # Extract page range
            from_page = None
            to_page = None
            if 'startPage' in search_result:
                sp = search_result.get('startPage')
                ep = search_result.get('endPage')
                if sp and int(sp) > 0:
                    from_page = int(sp)
                if ep and int(ep) > 0:
                    to_page = int(ep)

                if from_page and not to_page:
                    to_page = from_page
                if to_page and not from_page:
                    from_page = to_page

            # 5) Collect data based on search type
            # NOTE: í•­ìƒ ìƒˆë¡œ ê²€ìƒ‰ (previous_contextì—ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ìˆìŒ)
            # usePreviousContext=true ì´ë©´ ì´ì „ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¡œ ì¬ê²€ìƒ‰
            book_texts = ""
            texts_summary = ""
            db_time_ms = 0
            pages_found = []
            search_logs = []
            chunks = []
            visual_elements = []

            core_keywords = search_result.get('coreKeywords', [])
            sub_keywords = search_result.get('subKeywords', [])
            search_type = search_result.get('searchType', 'keyword')
            pages_spec = search_result.get('pages', '')

            if use_previous_context:
                self.logger.info(f'ğŸ”„ Re-searching with previous metadata: {search_type}, pages={pages_spec}')

            # í•­ìƒ ê²€ìƒ‰ ìˆ˜í–‰ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
            if True:  # ê¸°ì¡´ ì¡°ê±´ë¬¸ ì œê±°
                if search_type == "irrelevant":
                    # ì±…ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ - ê²€ìƒ‰ ìƒëµ
                    book_texts = ""
                    texts_summary = ""
                    pages_found = []
                    self.logger.info(f'âš ï¸ Irrelevant question detected (stream): {user_search}')

                elif search_type == "summary":
                    texts_summary = await self._get_toc_summaries_by_page_spec(record['id'], pages_spec)
                    # Parse pages_spec to populate pages_found for source citation (supports multiple ranges like "580-582,628,687-688")
                    pages_found = self._parse_page_spec(pages_spec) if pages_spec else []

                elif search_type == "toc":
                    book_texts = ""
                    pages_found = []

                elif search_type == "semantic":
                    # Qdrant ì‹œë§¨í‹± ê²€ìƒ‰
                    db_search_start = time.time()
                    try:
                        qdrant = get_qdrant_service()
                        semantic_results = await qdrant.search(
                            pdf_id=record['id'],
                            query=user_search,
                            limit=5
                        )
                        db_time_ms = round((time.time() - db_search_start) * 1000, 3)

                        if semantic_results:
                            chunks = []
                            for result in semantic_results:
                                chunks.append({
                                    'page': result['page_number'],
                                    'content': result['content']
                                })
                            pages_found = sorted(set(c['page'] for c in chunks))

                            if len(pages_found) <= 20:
                                book_texts = "\n\n".join([
                                    f"page: {c['page']}\n\në‚´ìš©: {c['content'].strip()[:800]}"
                                    for c in chunks[:5]
                                ])
                            else:
                                book_texts = ""
                        else:
                            book_texts = ""
                            pages_found = []

                        self.logger.info(f"ğŸ” Semantic search found {len(chunks)} chunks")

                        # ğŸ“Š ë¡œê¹…: DB Search (Qdrant)
                        query_logger.log_db_search(
                            method="qdrant",
                            query=user_search,
                            time_ms=db_time_ms,
                            results_count=len(chunks),
                            pages_found=pages_found,
                            extra_data={
                                "qdrant_top_k": 5,
                                "qdrant_fallback_to_keyword": False
                            }
                        )
                        self.logger.debug(f"ğŸ“Š [QueryLog] Qdrant search: {len(chunks)} results in {db_time_ms}ms")

                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Semantic search failed, falling back to keyword: {e}")

                        # ğŸ“Š ë¡œê¹…: Qdrant Fallback
                        query_logger.log_db_search(
                            method="qdrant",
                            query=user_search,
                            time_ms=db_time_ms if 'db_time_ms' in locals() else 0,
                            results_count=0,
                            pages_found=[],
                            extra_data={
                                "qdrant_fallback_to_keyword": True,
                                "error": str(e)
                            }
                        )

                        search_type = "keyword"  # Fallback to keyword search
                        # Continue to keyword search below

                else:
                    has_keywords = len(core_keywords) > 0

                    if not has_keywords:
                        if not pages_spec:
                            pages_spec = f"{from_page}-{to_page}" if from_page else "1-5"

                        db_search_start = time.time()
                        chunks = await self._get_chunks_text_by_page_spec(record['id'], pages_spec)
                        db_time_ms = round((time.time() - db_search_start) * 1000, 3)

                        if chunks:
                            pages_found = sorted(set(c['page'] for c in chunks))

                            # ğŸ“Š ë¡œê¹…: DB Search (Page Spec)
                            query_logger.log_db_search(
                                method="page_spec",
                                query=f"pages:{pages_spec}",
                                time_ms=db_time_ms,
                                results_count=len(chunks),
                                pages_found=pages_found
                            )
                            self.logger.debug(f"ğŸ“Š [QueryLog] Page spec search: {len(chunks)} results from pages {pages_spec}")

                            if len(pages_found) <= 20:
                                # LOW EFFORT: ê° ì²­í¬ ë‚´ìš©ì„ 800ìë¡œ ì œí•œ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê°ì†Œ)
                                book_texts = "\n\n".join([
                                    f"page: {c['page']}\n\në‚´ìš©: {c['content'].strip()[:800]}"
                                    for c in chunks[:5]  # ìµœëŒ€ 5ê°œ ì²­í¬ë§Œ
                                ])
                            else:
                                book_texts = ""

                    else:
                        pages_search = self._parse_page_spec(pages_spec) if pages_spec else None

                        db_search_start = time.time()
                        result = await self._fetch_results_with_keywords(
                            pdf_id=record['id'],
                            core_keywords=core_keywords,
                            sub_keywords=sub_keywords,
                            pages=pages_search
                        )
                        db_time_ms = round((time.time() - db_search_start) * 1000, 3)

                        chunks = result['chunks']
                        visual_elements = result.get('visualElements', [])
                        search_logs = result.get('searchLogs', [])

                        fallback_triggered = False

                        # ğŸ“Œ Fallback: If no results found with page restriction, retry without pages
                        if not chunks and pages_search:
                            self.logger.info(f"ğŸ”„ No results in pages {pages_spec}, retryingå…¨ all pages...")
                            fallback_start = time.time()
                            result = await self._fetch_results_with_keywords(
                                pdf_id=record['id'],
                                core_keywords=core_keywords,
                                sub_keywords=sub_keywords,
                                pages=None  # Search ALL pages
                            )
                            chunks = result['chunks']
                            visual_elements = result.get('visualElements', [])
                            db_time_ms += round((time.time() - fallback_start) * 1000, 3)
                            fallback_triggered = True
                            if chunks:
                                self.logger.info(f"âœ… Fallback found {len(chunks)} chunks without page restriction")

                        if chunks:
                            pages_found = sorted(set(c['page'] for c in chunks))

                            # ğŸ“Š ë¡œê¹…: DB Search (FTS5)
                            core_kw_list = [k['keyword'] for k in core_keywords]
                            sub_kw_list = [k['keyword'] for k in sub_keywords]
                            query_logger.log_db_search(
                                method="fts5",
                                query=f"core:{core_kw_list} sub:{sub_kw_list}",
                                time_ms=db_time_ms,
                                results_count=len(chunks),
                                pages_found=pages_found,
                                extra_data={
                                    "fts5_keywords_used": {
                                        "core": core_kw_list,
                                        "sub": sub_kw_list
                                    },
                                    "fts5_page_restriction": pages_spec if pages_spec else "none",
                                    "fts5_fallback_triggered": fallback_triggered
                                }
                            )
                            self.logger.debug(f"ğŸ“Š [QueryLog] FTS5 search: {len(chunks)} results, fallback={fallback_triggered}")

                            if len(pages_found) <= 20:
                                # LOW EFFORT: ê° ì²­í¬ ë‚´ìš©ì„ 800ìë¡œ ì œí•œ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê°ì†Œ)
                                book_texts = "\n\n".join([
                                    f"page: {c['page']}\n\në‚´ìš©: {c['content'].strip()[:800]}"
                                    for c in chunks[:5]  # ìµœëŒ€ 5ê°œ ì²­í¬ë§Œ
                                ])
                            else:
                                book_texts = ""

            # 6) LLM Call #2: Stream answer (NO caching)
            answer_chunks = []
            answer_time_start = time.time()

            if user_search:
                self.logger.info(f"ğŸ”„ Streaming answer for: {user_search[:50]}")

                # Stream the answer based on search type
                if search_type == "irrelevant":
                    # ì±…ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ - ì•ˆë‚´ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
                    async def irrelevant_stream():
                        message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì€ í˜„ì¬ ì„ íƒí•˜ì‹  êµì¬ '{record['title']}'ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ì—†ì–´ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        yield message
                    stream = irrelevant_stream()
                elif search_type == "summary":
                    stream = llm_service.get_answer_stream(
                        title=record['title'],
                        texts=texts_summary,
                        history_user_query=conversation_context_formatted,
                        question=user_search,
                        toc=""
                    )
                elif search_type == "toc":
                    stream = llm_service.get_answer_stream(
                        title=record['title'],
                        texts="",
                        history_user_query=conversation_context_formatted,
                        question=user_search,
                        toc=toc  # Use core_summary (includes keywords)
                    )
                else:
                    stream = llm_service.get_answer_stream(
                        title=record['title'],
                        texts=book_texts,
                        history_user_query=conversation_context_formatted,
                        question=user_search,
                        toc=toc  # Use core_summary (includes keywords)
                    )

                # Stream chunks to client
                async for chunk in stream:
                    answer_chunks.append(chunk)
                    yield {
                        'chunk': chunk,
                        'done': False
                    }

            answer_time_ms = round((time.time() - answer_time_start) * 1000, 1)
            answer = ''.join(answer_chunks)

            # ğŸ“Š ë¡œê¹…: LLM Answer Generation
            prompt_length = len(book_texts) + len(toc) + len(conversation_context_formatted) + len(user_search)
            query_logger.log_llm_answer(
                request_time_ms=answer_time_ms,
                prompt_length=prompt_length,
                chunks_count=len(chunks),
                answer_length=len(answer)
            )
            self.logger.debug(f"ğŸ“Š [QueryLog] LLM answer: {len(answer)} chars in {answer_time_ms}ms")

            # ìë™ ì¶œì²˜ ìƒì„± (LLMì´ ì•„ë‹Œ ì‹œìŠ¤í…œì—ì„œ)
            source_citation = await self._generate_source_citation(
                pdf_id=record['id'],
                pages_found=pages_found,
                search_type=search_type
            )
            if source_citation:
                # âœ… Stream the source citation to client
                yield {
                    'chunk': source_citation,
                    'done': False
                }
                answer = answer + source_citation

            # Fetch visual elements (images, diagrams, tables) for found pages
            if pages_found:
                try:
                    visual_elements = await pdf_database.get_visual_elements(
                        pdf_id=record['id'],
                        page_numbers=pages_found
                    )
                    self.logger.info(f"ğŸ“Š Found {len(visual_elements)} visual elements on pages {pages_found[:3]}...")
                except Exception as e:
                    self.logger.error(f"âŒ Failed to fetch visual elements: {e}")
                    visual_elements = []

            # 7) Build metadata
            search_token = search_info_from_llm['token']
            core_keyword_str = ' '.join([f'"{k["keyword"]}"' for k in core_keywords]) if core_keywords else ""
            sub_keyword_str = ' '.join([f'"{k["keyword"]}"' for k in sub_keywords]) if sub_keywords else ""

            keyword_str = ""
            if core_keyword_str or sub_keyword_str:
                keyword_str = f"í•µì‹¬: {core_keyword_str}"
                if sub_keyword_str:
                    keyword_str += f" / ë³´ì¡°: {sub_keyword_str}"

            infos = {
                'userid': userid,
                'sessionid': sessionid,
                'ì œëª©': record['title'],
                'isbn': record['isbn'],
                'ìœ ì €ë°œí™”': query,
                'selectText': selected_text or None,
                'ê²€ìƒ‰ìœ í˜•': search_result.get('searchType'),
                'ê²€ìƒ‰í‚¤ì›Œë“œ': keyword_str if keyword_str else None,
                'í˜ì´ì§€ë²”ìœ„': pages_spec or None,
                'ê²€ìƒ‰ëœí˜ì´ì§€': pages_found if pages_found else [],
                'ê²€ìƒ‰ì„ íƒì´ìœ ': search_result.get('reason'),
                'ê²€ìƒ‰ìš”ì²­LLMì‹œê°„': llm_time_ms,
                'ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰ì‹œê°„': db_time_ms,
                'ë‹µë³€ìƒì„±ì‹œê°„': answer_time_ms,
                'answer': answer or None,
            }

            # Save context for followup questions
            # NOTE: texts/chunks ë“± ì›ë¬¸ ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í­ë°œ ë°©ì§€)
            # í•„ìš”ì‹œ ë©”íƒ€ë°ì´í„°ë¡œ ì¬ê²€ìƒ‰
            context_info = {
                'searchType': search_result.get('searchType'),
                'pages': pages_spec,
                'coreKeywords': core_keywords,
                'subKeywords': sub_keywords,
                'pagesFound': pages_found,
                'query': query  # ì§ì „ ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
                # 'texts': book_texts,  # âŒ ì œê±° - ì¬ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
                # 'texts_summary': texts_summary,  # âŒ ì œê±°
                # 'chunks': chunks,  # âŒ ì œê±°
                # 'visualElements': visual_elements  # âŒ ì œê±°
            }

            context_key = f"{userid}:{sessionid}"
            self.context_store[context_key] = context_info
            self.logger.info(f'ğŸ’¾ Saved context (metadata only) for session: {sessionid}')

            # Save conversation history
            await self.conversations.append(sessionid, [
                {'role': 'user', 'content': query},
                {'role': 'assistant', 'content': answer}
            ])
            self.logger.info(f'ğŸ’¬ Saved conversation to cache: {sessionid}')

            # Save conversation to file
            await self.conversation_logger.log_conversation(
                session_id=sessionid,
                user_id=userid,
                isbn=isbn,
                query=query,
                response=answer,
                metadata={
                    'search_type': search_result.get('searchType'),
                    'pages': pages_spec,
                    'pages_found': pages_found,
                    'response_length': len(answer),
                    'streaming': True
                }
            )

            # Update conversation summary + recent turns (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ - ì‘ë‹µ ë¸”ë¡ ì•ˆ í•¨)
            asyncio.create_task(self._update_conversation_summary(
                sessionid=sessionid,
                user_query=query,
                assistant_response=answer,
                isbn=isbn
            ))

            # ğŸ’¾ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µë„ ìºì‹œì— ì €ì¥ (ë¬¸ë§¥ ë…ë¦½ì ì¸ ì§ˆë¬¸ë§Œ)
            if not is_context_dependent:
                full_cache_context = f"full:{isbn}"
                cache_data = {
                    'answer': answer,
                    'infos': infos,
                    'chunks': chunks,
                    'visualElements': visual_elements
                }
                self.llm_cache.set(user_search, full_cache_context, cache_data)
                self.logger.info(f"ğŸ’¾ Cached streaming response: {user_search[:50]}")

            # ğŸ“Š Save query log to DB (streaming)
            try:
                await query_logger.save_log(
                    answer=answer or '',
                    chunks_count=len(chunks),
                    visual_elements_count=len(visual_elements)
                )
                self.logger.info(f"ğŸ“Š [QueryLog] Saved streaming log to DB")
            except Exception as log_error:
                self.logger.error(f"âŒ Failed to save query log: {log_error}")

            # â±ï¸ ì´ ì†Œìš” ì‹œê°„ ê³„ì‚°
            total_time_ms = (time.time() - start_time) * 1000

            # Final message with metadata
            yield {
                'chunk': '',
                'done': True,
                'metadata': {
                    'infos': infos,
                    'chunks': chunks,
                    'visualElements': visual_elements,
                    'total_time_ms': round(total_time_ms, 2)
                }
            }

        except Exception as e:
            self.logger.error(f"Error in streaming handler: {e}", exc_info=True)
            yield {
                'chunk': '',
                'done': True,
                'error': str(e)
            }

    # ========== Helper Methods ==========

    async def _generate_source_citation(self, pdf_id: str, pages_found: List[int], search_type: str) -> str:
        """
        ìë™ìœ¼ë¡œ ì¶œì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (LLMì´ ì•„ë‹Œ ì‹œìŠ¤í…œì—ì„œ ìƒì„±)

        Args:
            pdf_id: PDF ID
            pages_found: ê²€ìƒ‰ëœ í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            search_type: ê²€ìƒ‰ íƒ€ì…

        Returns:
            í˜•ì‹í™”ëœ ì¶œì²˜ ë¬¸ìì—´ (ì˜ˆ: "1-1 ì£¼ì œì–´ ì‚´í´ë³´ê¸° (19-26í˜ì´ì§€)")
        """
        if not pages_found or search_type in ["irrelevant", "followup"]:
            return ""

        try:
            # ëª©ì°¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            toc_entries = await pdf_database.get_toc_entries(pdf_id)
            if not toc_entries:
                return ""

            # í˜ì´ì§€ë¥¼ ëª©ì°¨ í•­ëª©ì— ë§¤í•‘
            page_to_toc = {}
            for entry in toc_entries:
                start = entry.get('start_page')
                end = entry.get('end_page')
                title = entry.get('title', '')

                if start and end:
                    for page in range(start, end + 1):
                        if page not in page_to_toc:
                            page_to_toc[page] = title

            # ê²€ìƒ‰ëœ í˜ì´ì§€ë“¤ì„ ëª©ì°¨ í•­ëª©ë³„ë¡œ ê·¸ë£¹í™”
            toc_groups = {}
            for page in sorted(pages_found):
                toc_title = page_to_toc.get(page)
                if toc_title:
                    if toc_title not in toc_groups:
                        toc_groups[toc_title] = []
                    toc_groups[toc_title].append(page)

            if not toc_groups:
                return ""

            # ì¶œì²˜ ë¬¸ìì—´ ìƒì„±
            sources = []
            for toc_title, pages in toc_groups.items():
                if len(pages) == 1:
                    sources.append(f"{toc_title} ({pages[0]}í˜ì´ì§€)")
                else:
                    # ì—°ì†ëœ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³‘í•©
                    ranges = []
                    start = pages[0]
                    end = pages[0]

                    for i in range(1, len(pages)):
                        if pages[i] == end + 1:
                            end = pages[i]
                        else:
                            if start == end:
                                ranges.append(f"{start}")
                            else:
                                ranges.append(f"{start}-{end}")
                            start = pages[i]
                            end = pages[i]

                    # ë§ˆì§€ë§‰ ë²”ìœ„ ì¶”ê°€
                    if start == end:
                        ranges.append(f"{start}")
                    else:
                        ranges.append(f"{start}-{end}")

                    sources.append(f"{toc_title} ({', '.join(ranges)}í˜ì´ì§€)")

            if sources:
                return "\n\n**ì¶œì²˜:**\n" + "\n".join(f"- {s}" for s in sources)

            return ""

        except Exception as e:
            self.logger.error(f"Failed to generate source citation: {e}")
            return ""

    async def _get_conversations_by_session_with_context(self, userid: str, sessionid: str) -> List[Dict]:
        """
        Get conversation history from cache

        Returns formatted history for LLM context:
        - recentConversations: Last 10 Q&A pairs with full content
        - previousQuestions: Earlier questions (without answers to save tokens)
        """
        # Get history from conversation cache
        history = await self.conversations.get(sessionid)

        if not history or len(history) == 0:
            return []

        # Format for LLM: recent 10 pairs + older questions only
        recent_conversations = []
        previous_questions = []

        # Group into Q&A pairs
        for i in range(0, len(history), 2):
            if i + 1 < len(history):
                pair = {
                    'question': history[i]['content'],
                    'answer': history[i+1]['content']
                }

                if len(recent_conversations) < 10:
                    recent_conversations.append(pair)
                else:
                    previous_questions.append(history[i]['content'])

        return {
            'recentConversations': recent_conversations,
            'previousQuestions': previous_questions
        }

    async def _get_latest_conversation_context(self, userid: str, sessionid: str) -> Optional[Dict]:
        """
        Get previous search context for followup questions

        Returns METADATA ONLY (no texts to prevent prompt bloat):
        - searchType: Type of search performed
        - pages: Page range used
        - coreKeywords/subKeywords: Keywords used
        - pagesFound: Pages that were found
        - query: Last user question

        NOTE: texts/chunks are NOT stored - will re-fetch if needed
        """
        context_key = f"{userid}:{sessionid}"
        return self.context_store.get(context_key)

    async def _get_toc_core_summary_text(self, pdf_id: str) -> str:
        """Get TOC with core summary (replaces getTocCoreSummaryText)"""
        return await pdf_database.get_toc_core_summary_text(pdf_id)

    async def _get_toc_simple(self, pdf_id: str) -> str:
        """Get simple TOC (replaces getTocSimple)"""
        return await pdf_database.get_toc_simple(pdf_id)

    async def _get_toc_chapters_only(self, pdf_id: str) -> str:
        """Get TOC with chapters only (no subsections)"""
        return await pdf_database.get_toc_chapters_only(pdf_id)

    async def _get_toc_summaries_by_page_spec(self, pdf_id: str, page_spec: str) -> str:
        """Get TOC summaries for page range (replaces getTocSummariesByPageSpec)"""
        return await pdf_database.get_toc_summaries_by_page_spec(pdf_id, page_spec)

    async def _get_chunks_text_by_page_spec(self, pdf_id: str, page_spec: str) -> List[Dict]:
        """Get text chunks by page spec (replaces getChunksTextByPageSpec)"""
        pages = self._parse_page_spec(page_spec)
        if not pages:
            return []

        # ë‹¨ì¼ ì¿¼ë¦¬ë¡œ í˜ì´ì§€ ë²”ìœ„ ì¡°íšŒ (N+1 ì¿¼ë¦¬ ë°©ì§€)
        min_page = min(pages)
        max_page = max(pages)
        all_pages = await pdf_database.get_pages_range(pdf_id, min_page, max_page)

        # ìš”ì²­ëœ í˜ì´ì§€ë§Œ í•„í„°ë§
        page_set = set(pages)
        chunks = [
            {'page': p['page_number'], 'content': p['content']}
            for p in all_pages
            if p['page_number'] in page_set and p['content']
        ]

        return chunks

    def _parse_page_spec(self, page_spec: str) -> List[int]:
        """
        Parse page specification like "1,2,10-20,55"
        Returns list of page numbers
        """
        if not page_spec:
            return []

        pages = []
        parts = page_spec.split(',')

        for part in parts:
            part = part.strip()
            if '-' in part:
                # Range like "10-20"
                start, end = part.split('-')
                try:
                    start_num = int(start.strip())
                    end_num = int(end.strip())
                    pages.extend(range(start_num, end_num + 1))
                except ValueError:
                    continue
            else:
                # Single page
                try:
                    pages.append(int(part))
                except ValueError:
                    continue

        return sorted(set(pages))

    async def _fetch_results_with_keywords(
        self,
        pdf_id: str,
        core_keywords: List[Dict],
        sub_keywords: List[Dict],
        pages: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        Fetch results using FTS search with keywords
        Replaces pdfService.fetchResults()

        Enhanced with error handling for comparison queries
        """
        try:
            # Build FTS query
            # Core keywords: AND (must all be present)
            # Sub keywords: OR (boost score if present)

            core_terms = []
            for kw in core_keywords:
                keyword = kw['keyword']
                alternatives = kw.get('alternatives', [])
                # Add keyword and all alternatives
                all_terms = [keyword] + alternatives
                core_terms.append(all_terms)

            sub_terms = []
            for kw in sub_keywords:
                keyword = kw['keyword']
                alternatives = kw.get('alternatives', [])
                all_terms = [keyword] + alternatives
                sub_terms.extend(all_terms)

            # Perform FTS search with error handling
            results = await pdf_database.search_pdf_content(
                pdf_id=pdf_id,
                core_keywords=core_terms,
                sub_keywords=sub_terms,
                page_numbers=pages
            )

            chunks = []
            for r in results:
                chunks.append({
                    'page': r['page_number'],
                    'content': r['content']
                })

            self.logger.info(f"Keyword search found {len(chunks)} chunks")

            return {
                'chunks': chunks,
                'visualElements': [],
                'searchLogs': [f"Found {len(chunks)} chunks"]
            }

        except Exception as e:
            self.logger.error(f"Error in keyword search: {e}", exc_info=True)
            # Return empty results instead of crashing
            return {
                'chunks': [],
                'visualElements': [],
                'searchLogs': [f"Search failed: {str(e)}"]
            }

    async def _get_conversation_summary(self, sessionid: str) -> Dict[str, any]:
        """
        ì„¸ì…˜ì˜ ëŒ€í™” ìš”ì•½ ë° ìµœê·¼ ëŒ€í™” ê°€ì ¸ì˜¤ê¸°

        Returns:
            {
                "summary": str,  # ì „ì²´ ëŒ€í™” ìš”ì•½ (3-5ë¬¸ì¥)
                "recent_turns": List[Dict]  # ìµœê·¼ Ní„´ [{user: ..., assistant: ...}]
            }
        """
        if sessionid not in self.conversation_summaries:
            return {
                "summary": "",
                "recent_turns": []
            }
        return self.conversation_summaries[sessionid]

    async def _format_conversation_context(self, sessionid: str) -> str:
        """
        ëŒ€í™” ìš”ì•½ + ìµœê·¼ ëŒ€í™”ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…

        Returns:
            í¬ë§·íŒ…ëœ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ëŒ€í™” ìš”ì•½ + ìµœê·¼ ëŒ€í™”)
        """
        conv_data = await self._get_conversation_summary(sessionid)
        summary = conv_data["summary"]
        recent_turns = conv_data["recent_turns"]

        # ë¹ˆ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        if not summary and not recent_turns:
            return ""

        # í¬ë§·íŒ…
        result = ""

        # 1) ëŒ€í™” ìš”ì•½ ë¸”ë¡
        if summary:
            result += f"[ëŒ€í™” ìš”ì•½]\n{summary}\n\n"

        # 2) ìµœê·¼ ëŒ€í™” ë¸”ë¡ (ìµœê·¼ 3í„´ë§Œ í‘œì‹œ)
        if recent_turns:
            result += "[ìµœê·¼ ëŒ€í™”]\n"
            display_turns = recent_turns[-3:]  # ìµœê·¼ 3í„´ë§Œ í‘œì‹œ
            for i, turn in enumerate(display_turns, 1):
                user_q = turn["user"][:100]  # ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½
                asst_a = turn["assistant"][:150]  # ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½
                result += f"{i}. ì‚¬ìš©ì: {user_q}\n   ë‹µë³€: {asst_a}\n"

        return result.strip()

    async def _update_conversation_summary(
        self,
        sessionid: str,
        user_query: str,
        assistant_response: str,
        isbn: str
    ):
        """
        ëŒ€í™” ìš”ì•½ ë° ìµœê·¼ ëŒ€í™” ì—…ë°ì´íŠ¸

        1. recent_turnsì— ìƒˆ í„´ ì¶”ê°€ (ìµœëŒ€ MAX_RECENT_TURNSê°œ ìœ ì§€)
        2. LLMìœ¼ë¡œ ì „ì²´ ëŒ€í™” ìš”ì•½ ìƒì„±/ê°±ì‹ 
        """
        # ê¸°ì¡´ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        current_data = await self._get_conversation_summary(sessionid)
        current_summary = current_data["summary"]
        recent_turns = current_data["recent_turns"]

        # 1. recent_turns ì—…ë°ì´íŠ¸ (deque ë™ì‘)
        new_turn = {
            "user": user_query,
            "assistant": assistant_response
        }
        recent_turns.append(new_turn)

        # ìµœëŒ€ ê°œìˆ˜ ìœ ì§€
        if len(recent_turns) > self.MAX_RECENT_TURNS:
            recent_turns = recent_turns[-self.MAX_RECENT_TURNS:]

        # 2. ëŒ€í™” ìš”ì•½ ê°±ì‹  (LLM ì‚¬ìš©)
        try:
            new_summary = await self._generate_conversation_summary(
                current_summary=current_summary,
                recent_turns=recent_turns,
                isbn=isbn
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëŒ€í™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ê¸°ì¡´ ìš”ì•½ ìœ ì§€: {e}")
            new_summary = current_summary

        # 3. ì €ì¥
        self.conversation_summaries[sessionid] = {
            "summary": new_summary,
            "recent_turns": recent_turns
        }

        self.logger.info(f"ğŸ“ Updated conversation summary for {sessionid} (summary: {len(new_summary)} chars, turns: {len(recent_turns)})")

    async def _generate_conversation_summary(
        self,
        current_summary: str,
        recent_turns: List[Dict],
        isbn: str
    ) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ì „ì²´ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

        Args:
            current_summary: í˜„ì¬ ìš”ì•½ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
            recent_turns: ìµœê·¼ ëŒ€í™” í„´ë“¤
            isbn: ë„ì„œ ISBN (ì»¨í…ìŠ¤íŠ¸ìš©)

        Returns:
            ìƒˆë¡œìš´ ìš”ì•½ ë¬¸ìì—´
        """
        # ìµœê·¼ ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        recent_text = ""
        for i, turn in enumerate(recent_turns[-3:], 1):  # ìµœê·¼ 3í„´ë§Œ ìš”ì•½ì— ì‚¬ìš©
            recent_text += f"{i}. ì‚¬ìš©ì: {turn['user']}\n"
            recent_text += f"   ë‹µë³€: {turn['assistant'][:200]}...\n\n"  # ë‹µë³€ì€ 200ìë§Œ

        # ìš”ì•½ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ëŒ€í™”ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ê¸°ì¡´ ìš”ì•½:
{current_summary if current_summary else '(ì—†ìŒ)'}

ìµœê·¼ ëŒ€í™”:
{recent_text}

ìš”êµ¬ì‚¬í•­:
- ê¸°ì¡´ ìš”ì•½ì´ ìˆìœ¼ë©´ ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì—…ë°ì´íŠ¸
- ì‚¬ìš©ìê°€ ì£¼ë¡œ ë¬¼ì–´ë³¸ ì£¼ì œì™€ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ
- 3-5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ ì œì™¸

ìš”ì•½:"""

        try:
            result = await llm_service.generate_text(
                prompt=prompt,
                mode="chat"
            )
            summary = result['result'].strip()

            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 500ì)
            if len(summary) > 500:
                summary = summary[:500] + "..."

            return summary

        except Exception as e:
            self.logger.error(f"âŒ Summary generation failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìš”ì•½ ë°˜í™˜
            return current_summary


# Create singleton instance
qa_service = QAService()
