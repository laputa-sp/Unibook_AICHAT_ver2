"""
LLM Service - Multi-Engine Support (Ollama + vLLM)
Replaces Google Gemini API calls with local LLM services

This service supports:
- Ollama: gpt-oss:20b via AsyncClient
- vLLM: OpenAI-compatible API endpoint

Engine can be switched via LLM_ENGINE config or per-call engine parameter.

Includes comprehensive metrics collection for performance comparison.
"""
import logging
import asyncio
from typing import List, Dict, Optional, Any
from ollama import AsyncClient
import json
import httpx

from app.config import settings
from app.utils.llm_metrics import llm_metrics

logger = logging.getLogger(__name__)


class LLMService:
    """
    LLM Service with Multi-Engine Support

    Supports both Ollama and vLLM engines.
    Engine selection via settings.LLM_ENGINE or per-call engine parameter.

    Replaces all Gemini API calls from the original Node.js implementation:
    - gemini.js -> generateText()
    - getAnswer.js -> get_answer()
    - getSummary.js -> get_summary()
    - getImageToText.js -> get_image_to_text()
    - getSearchType.js -> get_search_type()
    - getFormatToc.js -> get_format_toc()
    """

    def __init__(self):
        # Ollama client
        self.ollama_client = AsyncClient(host=settings.OLLAMA_BASE_URL)
        self.ollama_model = settings.OLLAMA_MODEL or "gpt-oss:20b"

        # vLLM client (OpenAI-compatible)
        self.vllm_base_url = settings.VLLM_BASE_URL
        self.vllm_model = settings.VLLM_MODEL_NAME
        self.vllm_api_key = settings.VLLM_API_KEY

        # Default engine from settings
        self.default_engine = settings.LLM_ENGINE.lower()

        # Fallback configuration
        self.enable_fallback = settings.ENABLE_LLM_FALLBACK
        self.fallback_engine = settings.FALLBACK_ENGINE.lower()

        # Legacy compatibility
        self.client = self.ollama_client
        self.model = self.ollama_model

        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸš€ LLM Service initialized with default engine: {self.default_engine}")
        self.logger.info(f"   Ollama: {settings.OLLAMA_BASE_URL} ({self.ollama_model})")
        self.logger.info(f"   vLLM: {self.vllm_base_url} ({self.vllm_model})")
        self.logger.info(f"   Fallback: {'enabled' if self.enable_fallback else 'disabled'} (â†’ {self.fallback_engine})")

    def _clean_llm_response(self, text: str) -> str:
        """
        Clean LLM response by removing prompt artifacts and instructions

        Removes common issues where LLM echoes back prompt instructions
        """
        if not text:
            return text

        # Remove leading meta-descriptions (model explaining what it's doing)
        # Common pattern: "We need answer short, 200-400 chars. Provide..."
        import re

        # Pattern 1: English meta instructions at the start
        # Matches things like "We need answer short" or "Provide concept description"
        text = re.sub(
            r'^(We need .{0,100}?\.|Provide .{0,100}?\.|Answer should .{0,100}?\.)+\s*',
            '',
            text,
            flags=re.IGNORECASE | re.MULTILINE
        )

        # Pattern 2: Inline prompt leakage (English instructions before Korean content)
        # Matches: "We need answer short, 200-400 chars. Provide...ìê¸°íš¨ëŠ¥ê°ì€"
        text = re.sub(
            r'^.*?([ê°€-í£])',
            r'\1',
            text,
            count=1
        )

        # Remove lines that look like prompt instructions
        lines = text.split('\n')
        cleaned_lines = []

        skip_patterns = [
            'ì¶œë ¥ í˜•ì‹:',
            'ì¶œë ¥í˜•ì‹:',
            'ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆ',
            'ì‘ë‹µ ì˜ˆì‹œ:',
            'ì¤‘ìš”: ë°˜ë“œì‹œ',
            'JSONë§Œ ì¶œë ¥',
            'ìŠ¤í‚¤ë§ˆ:',
            'ì¶œë ¥ ì˜ˆì‹œ',
            'We need',
            'Provide concept',
            'Answer should',
        ]

        for line in lines:
            # Skip lines that match instruction patterns
            if any(pattern in line for pattern in skip_patterns):
                continue
            cleaned_lines.append(line)

        result = '\n'.join(cleaned_lines).strip()

        # Remove leading/trailing artifacts
        if result.startswith('ì¶œë ¥:') or result.startswith('ì‘ë‹µ:'):
            result = result.split(':', 1)[1].strip()

        return result

    def _extract_json_from_text(self, text: str) -> Dict[str, Any]:
        """
        Enhanced JSON extraction with multiple strategies

        Tries multiple approaches to extract valid JSON from LLM response:
        1. Direct parsing
        2. Remove markdown code blocks
        3. Find outermost braces with nesting support
        4. Multiple regex patterns
        """
        import re

        if not text:
            self.logger.error("Empty text for JSON extraction")
            raise ValueError("Empty response from LLM")

        # Strategy 1: Direct parsing
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Strategy 2: Remove markdown code blocks
        cleaned = text
        if '```' in text:
            # Remove ```json or ``` markers
            cleaned = re.sub(r'```(?:json)?\s*', '', text)
            cleaned = cleaned.strip()
            try:
                return json.loads(cleaned)
            except json.JSONDecodeError:
                pass

        # Strategy 3: Find outermost braces with proper nesting
        first_brace = text.find('{')
        if first_brace != -1:
            # Find matching closing brace
            depth = 0
            for i in range(first_brace, len(text)):
                if text[i] == '{':
                    depth += 1
                elif text[i] == '}':
                    depth -= 1
                    if depth == 0:
                        json_str = text[first_brace:i+1]
                        try:
                            return json.loads(json_str)
                        except json.JSONDecodeError:
                            break

        # Strategy 4: Multiple regex patterns
        patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # Nested braces
            r'\{.*?\}',  # Non-greedy
            r'\{.+\}',  # Greedy
        ]

        for pattern in patterns:
            matches = re.finditer(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match.group(0))
                except json.JSONDecodeError:
                    continue

        # All strategies failed
        self.logger.error(f"JSON extraction failed after all strategies")
        self.logger.error(f"Response text (first 500 chars): {text[:500]}")
        raise ValueError(f"Could not extract valid JSON from response")

    async def generate_text(
        self,
        prompt: str,
        response_schema: Optional[Dict] = None,
        mode: str = "chat",
        image_data: Optional[Dict] = None,
        max_retries: int = 2,
        engine: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Core LLM generation function with multi-engine support and automatic fallback

        Args:
            prompt: The prompt to send to LLM
            response_schema: Optional JSON schema for structured output
            mode: "chat", "vision", or "chat2" (mode handling varies by engine)
            image_data: Optional image data for vision tasks
            max_retries: Maximum retry attempts for empty responses (default: 2)
            engine: Engine to use ("ollama" or "vllm"). If None, uses default from settings.

        Returns:
            Dict with 'result', 'token' (usage metadata), 'duration' (ms)

        Raises:
            ValueError: If engine is unknown or both primary and fallback engines fail
        """
        # Determine which engine to use
        selected_engine = (engine or self.default_engine).lower()

        # Validate engine
        if selected_engine not in ["ollama", "vllm"]:
            raise ValueError(f"Unknown engine: {selected_engine}. Use 'ollama' or 'vllm'")

        # Try primary engine
        try:
            return await self._execute_engine_call(
                engine=selected_engine,
                prompt=prompt,
                response_schema=response_schema,
                mode=mode,
                image_data=image_data,
                max_retries=max_retries
            )

        except Exception as primary_error:
            # Check if fallback is enabled and appropriate
            should_fallback = (
                self.enable_fallback and
                self.fallback_engine != selected_engine and
                self.fallback_engine in ["ollama", "vllm"]
            )

            if not should_fallback:
                # No fallback configured or same engine, re-raise original error
                raise

            # Log and attempt fallback
            error_type = type(primary_error).__name__
            error_msg = str(primary_error)

            self.logger.warning(
                f"âš ï¸ [{selected_engine.upper()}] Engine failed: {error_type}: {error_msg}"
            )
            self.logger.warning(
                f"ğŸ”„ Attempting fallback to {self.fallback_engine.upper()} engine..."
            )

            # Record fallback metrics
            llm_metrics.log_fallback(
                from_engine=selected_engine,
                to_engine=self.fallback_engine,
                reason=error_type,
                original_error=error_msg
            )

            try:
                # Attempt fallback
                result = await self._execute_engine_call(
                    engine=self.fallback_engine,
                    prompt=prompt,
                    response_schema=response_schema,
                    mode=mode,
                    image_data=image_data,
                    max_retries=max_retries
                )

                self.logger.info(
                    f"âœ… Fallback successful: {self.fallback_engine.upper()} completed request"
                )

                # Mark fallback engine as healthy
                llm_metrics.update_engine_health(
                    engine=self.fallback_engine,
                    model=self.vllm_model if self.fallback_engine == "vllm" else self.ollama_model,
                    is_healthy=True
                )

                return result

            except Exception as fallback_error:
                # Both engines failed
                self.logger.error(
                    f"âŒ Fallback to {self.fallback_engine.upper()} also failed: {fallback_error}"
                )

                # Mark both engines as unhealthy
                llm_metrics.update_engine_health(
                    engine=selected_engine,
                    model=self.vllm_model if selected_engine == "vllm" else self.ollama_model,
                    is_healthy=False
                )
                llm_metrics.update_engine_health(
                    engine=self.fallback_engine,
                    model=self.vllm_model if self.fallback_engine == "vllm" else self.ollama_model,
                    is_healthy=False
                )

                # Re-raise the original error (more relevant)
                raise primary_error from fallback_error

    async def _execute_engine_call(
        self,
        engine: str,
        prompt: str,
        response_schema: Optional[Dict] = None,
        mode: str = "chat",
        image_data: Optional[Dict] = None,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        Execute LLM call on specified engine

        Internal method used by generate_text() to route to specific engine.

        Args:
            engine: "ollama" or "vllm"
            prompt: The prompt to send
            response_schema: Optional JSON schema
            mode: Operation mode
            image_data: Optional image data
            max_retries: Retry attempts

        Returns:
            Dict with 'result', 'token', 'duration'
        """
        if engine == "vllm":
            return await self._generate_text_vllm(
                prompt=prompt,
                response_schema=response_schema,
                mode=mode,
                max_retries=max_retries
            )
        elif engine == "ollama":
            return await self._generate_text_ollama(
                prompt=prompt,
                response_schema=response_schema,
                mode=mode,
                image_data=image_data,
                max_retries=max_retries
            )
        else:
            raise ValueError(f"Unknown engine: {engine}")

    async def _generate_text_ollama(
        self,
        prompt: str,
        response_schema: Optional[Dict] = None,
        mode: str = "chat",
        image_data: Optional[Dict] = None,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        Ollama-specific text generation

        Args:
            prompt: The prompt to send to LLM
            response_schema: Optional JSON schema for structured output
            mode: "chat", "vision", or "chat2"
            image_data: Optional image data for vision tasks
            max_retries: Maximum retry attempts

        Returns:
            Dict with 'result', 'token', 'duration'
        """
        import time

        # For vision mode with image, use vision-capable model
        model_name = self.ollama_model
        if mode == "vision" and image_data:
            model_name = "llava:latest"

        # Start metrics collection
        metrics_ctx = llm_metrics.log_request_start(
            engine="ollama",
            model=model_name,
            prompt_length=len(prompt),
            mode=mode,
            has_schema=response_schema is not None
        )

        # Retry loop for empty responses
        last_error = None
        for attempt in range(max_retries + 1):
            try:
                start_time = time.time()

                # Build request
                if response_schema:
                    # Request JSON output with detailed instructions
                    enhanced_prompt = f"""{prompt}

ì¶œë ¥ í˜•ì‹:
ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ìŠ¤í‚¤ë§ˆ:
{json.dumps(response_schema, ensure_ascii=False, indent=2)}

ì‘ë‹µ ì˜ˆì‹œ:
{{
  "field1": "value1",
  "field2": "value2"
}}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ê³ , ì•ë’¤ì— ì–´ë–¤ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

                    if attempt > 0:
                        self.logger.warning(f"ğŸ”„ [Ollama] Retry attempt {attempt}/{max_retries} for JSON response")

                    response = await self.ollama_client.generate(
                        model=model_name,
                        prompt=enhanced_prompt,
                        keep_alive=-1
                    )

                    response_text = response['response'].strip()
                    self.logger.debug(f"ğŸ“ [Ollama] Response length: {len(response_text)} chars")

                    result = self._extract_json_from_text(response_text)

                else:
                    # Plain text response
                    if attempt > 0:
                        self.logger.warning(f"ğŸ”„ [Ollama] Retry attempt {attempt}/{max_retries} for text response")

                    response = await self.ollama_client.generate(
                        model=model_name,
                        prompt=prompt,
                        keep_alive=-1
                    )

                    response_text = response['response'].strip()
                    self.logger.debug(f"ğŸ“ [Ollama] Response length: {len(response_text)} chars")

                    if not response_text:
                        raise ValueError("Empty response from LLM")

                    result = self._clean_llm_response(response_text)

                duration_ms = int((time.time() - start_time) * 1000)

                if attempt > 0:
                    self.logger.info(f"âœ… [Ollama] Retry successful on attempt {attempt}")
                break

            except ValueError as e:
                last_error = e
                if attempt < max_retries:
                    self.logger.warning(f"âš ï¸ [Ollama] Attempt {attempt + 1} failed: {str(e)}, retrying...")
                    self.logger.warning(f"ğŸ“Š Prompt length: {len(prompt)} chars, Model: {model_name}")
                    await asyncio.sleep(0.5)
                    continue
                else:
                    # Log failed request metrics
                    llm_metrics.log_request_end(
                        context=metrics_ctx,
                        completion_length=0,
                        token_usage=None,
                        error=str(e)
                    )
                    self.logger.error(f"âŒ [Ollama] All {max_retries + 1} attempts failed")
                    self.logger.error(f"ğŸ“Š Final attempt - Prompt length: {len(prompt)} chars")
                    raise

        # Prepare token info
        token_info = {
            'prompt_tokens': response.get('prompt_eval_count', 0),
            'completion_tokens': response.get('eval_count', 0),
            'total_tokens': response.get('prompt_eval_count', 0) + response.get('eval_count', 0)
        }

        # Calculate completion length
        completion_length = len(str(result)) if isinstance(result, str) else len(json.dumps(result, ensure_ascii=False))

        # Log successful request metrics
        llm_metrics.log_request_end(
            context=metrics_ctx,
            completion_length=completion_length,
            token_usage=token_info,
            error=None
        )

        return {
            'result': result,
            'token': token_info,
            'duration': duration_ms
        }

    async def _generate_text_vllm(
        self,
        prompt: str,
        response_schema: Optional[Dict] = None,
        mode: str = "chat",
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        vLLM-specific text generation via OpenAI-compatible API

        Args:
            prompt: The prompt to send to LLM
            response_schema: Optional JSON schema (currently text mode only)
            mode: "chat" mode (vision not yet supported)
            max_retries: Maximum retry attempts

        Returns:
            Dict with 'result', 'token', 'duration'
        """
        import time

        if mode == "vision":
            self.logger.warning("âš ï¸ [vLLM] Vision mode not yet supported, falling back to text mode")

        # Start metrics collection
        metrics_ctx = llm_metrics.log_request_start(
            engine="vllm",
            model=self.vllm_model,
            prompt_length=len(prompt),
            mode=mode,
            has_schema=response_schema is not None
        )

        # Build OpenAI-compatible request
        url = f"{self.vllm_base_url}/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
        }

        # Add API key if not "EMPTY"
        if self.vllm_api_key and self.vllm_api_key != "EMPTY":
            headers["Authorization"] = f"Bearer {self.vllm_api_key}"

        # Retry loop
        last_error = None
        for attempt in range(max_retries + 1):
            try:
                start_time = time.time()

                # Build messages
                user_content = prompt
                if response_schema:
                    # Add JSON schema instructions to prompt
                    user_content = f"""{prompt}

ì¶œë ¥ í˜•ì‹:
ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ìŠ¤í‚¤ë§ˆ:
{json.dumps(response_schema, ensure_ascii=False, indent=2)}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ê³ , ì•ë’¤ì— ì–´ë–¤ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

                    if attempt > 0:
                        self.logger.warning(f"ğŸ”„ [vLLM] Retry attempt {attempt}/{max_retries} for JSON response")

                # CRITICAL: Dynamic max_tokens based on prompt length (prevent overflow)
                # Korean text: ~1 char = 0.7 tokens (conservative estimate)
                estimated_prompt_tokens = int(len(user_content) * 0.7)
                model_context_limit = 8192
                available_tokens = model_context_limit - estimated_prompt_tokens

                if available_tokens < 200:
                    max_tokens = 100
                    self.logger.warning(f"âš ï¸ [vLLM] Very long prompt ({estimated_prompt_tokens} tokens), limiting max_tokens to {max_tokens}")
                else:
                    # ìœ ë™ì  max_tokens: JSONì€ ì¤‘ê°„, í…ìŠ¤íŠ¸ëŠ” ê¸¸ê²Œ (ë¬¸ì œ/ì„¤ëª… ëŒ€ì‘)
                    desired_max = 600 if response_schema else 1200
                    max_tokens = min(available_tokens - 200, desired_max)

                if max_tokens < 50:
                    self.logger.error(f"âŒ [vLLM] Prompt too long! Estimated: {estimated_prompt_tokens} tokens")
                    max_tokens = 50

                self.logger.info(f"ğŸ¯ [vLLM] Prompt: {len(user_content)} chars (~{estimated_prompt_tokens} tokens), max_tokens: {max_tokens} ({'JSON' if response_schema else 'text'})")

                payload = {
                    "model": self.vllm_model,
                    "messages": [
                        {"role": "user", "content": user_content}
                    ],
                    "temperature": 0.5,
                    "max_tokens": max_tokens,
                    "top_p": 0.85,
                }

                # Make HTTP request
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(url, json=payload, headers=headers)
                    response.raise_for_status()
                    data = response.json()

                # Extract response
                if "choices" not in data or len(data["choices"]) == 0:
                    raise ValueError("No choices in vLLM response")

                message = data["choices"][0]["message"]

                # ë””ë²„ê¹…: ë©”ì‹œì§€ ì „ì²´ êµ¬ì¡° í™•ì¸
                if attempt == 0:
                    self.logger.info(f"ğŸ” [vLLM] Message keys: {list(message.keys())}")

                response_text = message.get("content") or message.get("reasoning_content") or ""

                if not response_text:
                    self.logger.error(f"âŒ [vLLM] Empty response, full message: {message}")
                    raise ValueError("Empty response from vLLM")

                response_text = response_text.strip()
                self.logger.debug(f"ğŸ“ [vLLM] Response length: {len(response_text)} chars")

                # ë””ë²„ê¹…: JSON ì‘ë‹µì¸ ê²½ìš° ì²« 100ì ë¡œê·¸
                if response_schema and attempt == 0:
                    self.logger.info(f"ğŸ“„ [vLLM] JSON response preview: {response_text[:200]}...")

                # Parse response
                if response_schema:
                    result = self._extract_json_from_text(response_text)
                else:
                    if not response_text:
                        raise ValueError("Empty response from vLLM")
                    result = self._clean_llm_response(response_text)

                duration_ms = int((time.time() - start_time) * 1000)

                # Extract token usage
                usage = data.get("usage", {})
                token_info = {
                    'prompt_tokens': usage.get('prompt_tokens', 0),
                    'completion_tokens': usage.get('completion_tokens', 0),
                    'total_tokens': usage.get('total_tokens', 0)
                }

                if attempt > 0:
                    self.logger.info(f"âœ… [vLLM] Retry successful on attempt {attempt}")
                break

            except (httpx.HTTPError, ValueError) as e:
                last_error = e
                if attempt < max_retries:
                    self.logger.warning(f"âš ï¸ [vLLM] Attempt {attempt + 1} failed: {str(e)}, retrying...")
                    self.logger.warning(f"ğŸ“Š Prompt length: {len(prompt)} chars, Model: {self.vllm_model}")
                    await asyncio.sleep(0.5)
                    continue
                else:
                    # Log failed request metrics
                    llm_metrics.log_request_end(
                        context=metrics_ctx,
                        completion_length=0,
                        token_usage=None,
                        error=str(e)
                    )
                    self.logger.error(f"âŒ [vLLM] All {max_retries + 1} attempts failed")
                    self.logger.error(f"ğŸ“Š Final attempt - Prompt length: {len(prompt)} chars")
                    raise

        # Calculate completion length
        completion_length = len(str(result)) if isinstance(result, str) else len(json.dumps(result, ensure_ascii=False))

        # Log successful request metrics
        llm_metrics.log_request_end(
            context=metrics_ctx,
            completion_length=completion_length,
            token_usage=token_info,
            error=None
        )

        return {
            'result': result,
            'token': token_info,
            'duration': duration_ms
        }

    async def get_answer_stream(
        self,
        title: str,
        texts: str,
        history_user_query: str,
        question: str,
        toc: str,
        engine: Optional[str] = None
    ):
        """
        Stream answer generation for real-time user feedback

        Supports both vLLM and Ollama engines with automatic fallback.

        Args:
            title: Book title
            texts: Related book content
            history_user_query: Previous conversation history
            question: User's question
            toc: Table of contents
            engine: Engine to use ('vllm' or 'ollama'). If None, uses default_engine.

        Yields:
            Chunks of answer text as they're generated
        """
        prompt = self._build_answer_prompt(title, texts, history_user_query, question, toc)
        selected_engine = (engine or self.default_engine).lower()

        # Try primary engine first
        try:
            if selected_engine == "vllm":
                self.logger.debug(f"ğŸ¯ [vLLM] Starting streaming answer for: {question[:50]}")
                async for chunk in self._stream_answer_vllm(prompt):
                    yield chunk
                return
            elif selected_engine == "ollama":
                self.logger.debug(f"ğŸ¯ [Ollama] Starting streaming answer for: {question[:50]}")
                async for chunk in self._stream_answer_ollama(prompt, question):
                    yield chunk
                return
            else:
                raise ValueError(f"Unknown engine: {selected_engine}")

        except Exception as e:
            self.logger.warning(f"âš ï¸ [{selected_engine.upper()}] Streaming failed: {e}")

            # Try fallback if enabled
            if self.enable_fallback and self.fallback_engine != selected_engine:
                self.logger.warning(f"ğŸ”„ Attempting fallback to {self.fallback_engine.upper()} engine...")
                llm_metrics.log_fallback(
                    from_engine=selected_engine,
                    to_engine=self.fallback_engine,
                    reason=type(e).__name__,
                    error_message=str(e)
                )

                try:
                    if self.fallback_engine == "ollama":
                        async for chunk in self._stream_answer_ollama(prompt, question):
                            yield chunk
                        return
                    elif self.fallback_engine == "vllm":
                        async for chunk in self._stream_answer_vllm(prompt):
                            yield chunk
                        return
                except Exception as fallback_error:
                    self.logger.error(f"âŒ Fallback to {self.fallback_engine.upper()} also failed: {fallback_error}")
                    raise fallback_error
            else:
                raise

    async def _stream_answer_vllm(self, prompt: str):
        """
        Stream answer using vLLM engine

        Args:
            prompt: The formatted prompt

        Yields:
            Text chunks as they're generated
        """
        url = f"{self.vllm_base_url}/v1/chat/completions"
        headers = {"Content-Type": "application/json"}

        if self.vllm_api_key and self.vllm_api_key != "EMPTY":
            headers["Authorization"] = f"Bearer {self.vllm_api_key}"

        # CRITICAL: Conservative token estimation for Korean text
        # Korean text: ~1 char = 1 token (Unicode, CJK characters)
        # English text: ~1 char = 0.25 tokens (4 chars per token)
        # Mixed Korean/English with formatting: Use 1 char = 0.7 tokens to be VERY safe
        estimated_prompt_tokens = int(len(prompt) * 0.7)

        # Model context: 8192 tokens for gpt-oss:20b
        model_context_limit = 8192

        # Calculate available space for completion
        available_tokens = model_context_limit - estimated_prompt_tokens

        # Hard cap max_tokens to prevent overflow
        # vLLM will error if prompt + max_tokens > context_limit
        if available_tokens < 200:
            # Prompt is very long, use minimum viable max_tokens
            max_tokens = 100
            self.logger.warning(f"âš ï¸ [vLLM SSE] Very long prompt ({estimated_prompt_tokens} tokens), limiting max_tokens to {max_tokens}")
        else:
            # ìœ ë™ì  max_tokens: ê°„ë‹¨í•œ ë‹µë³€ë¶€í„° ê¸´ ì„¤ëª…/ë¬¸ì œê¹Œì§€ ëŒ€ì‘
            max_tokens = min(available_tokens - 200, 1200)

        # Absolute minimum check
        if max_tokens < 50:
            self.logger.error(f"âŒ [vLLM SSE] Prompt too long! Estimated: {estimated_prompt_tokens} tokens, limit: {model_context_limit}")
            max_tokens = 50  # Try anyway with minimum

        self.logger.info(f"ğŸ¯ [vLLM SSE] Prompt: {len(prompt)} chars (~{estimated_prompt_tokens} tokens), max_tokens: {max_tokens}, available: {available_tokens}")

        payload = {
            "model": self.vllm_model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.5,
            "max_tokens": max_tokens,
            "top_p": 0.85,
            "stream": True
        }

        async with httpx.AsyncClient(timeout=120.0) as client:
            async with client.stream("POST", url, json=payload, headers=headers) as response:
                # Check for errors before streaming
                if response.status_code != 200:
                    error_body = await response.aread()
                    self.logger.error(f"âŒ [vLLM SSE] HTTP {response.status_code}: {error_body.decode('utf-8')}")
                    self.logger.error(f"âŒ [vLLM SSE] Request payload: {json.dumps(payload, ensure_ascii=False)[:500]}")
                    response.raise_for_status()

                chunk_count = 0
                line_count = 0
                async for line in response.aiter_lines():
                    line_count += 1

                    if not line.strip() or line.startswith(":"):
                        continue

                    if line.startswith("data: "):
                        data_str = line[6:]  # Remove "data: " prefix

                        if data_str == "[DONE]":
                            self.logger.debug("ğŸ [vLLM SSE] Received [DONE] signal")
                            break

                        try:
                            data = json.loads(data_str)
                            if "choices" in data and len(data["choices"]) > 0:
                                delta = data["choices"][0].get("delta", {})

                                # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ delta ì¶œë ¥
                                if line_count <= 5:
                                    self.logger.info(f"ğŸ” [vLLM SSE] Delta #{line_count}: {delta}")

                                # Reasoning ëª¨ë¸: reasoning_content (ì‚¬ê³ ) + content (ë‹µë³€)
                                # âœ… reasoning_contentëŠ” SKIP (ì˜ì–´ ì‚¬ê³  ê³¼ì •)
                                # âœ… contentë§Œ ì¶œë ¥ (ì‹¤ì œ í•œêµ­ì–´ ë‹µë³€)
                                content = delta.get("content", "")

                                if content:
                                    chunk_count += 1
                                    if chunk_count <= 2:
                                        self.logger.info(f"âœ… [vLLM SSE] Content chunk #{chunk_count}: {content[:50]}")
                                    yield content
                                # reasoning_contentëŠ” ë¡œê·¸ë§Œ (ì¶œë ¥ ì•ˆ í•¨)
                                elif delta.get("reasoning_content"):
                                    if line_count <= 3:
                                        self.logger.debug(f"ğŸ§  [vLLM SSE] Skipping reasoning #{line_count}")
                        except json.JSONDecodeError as e:
                            if line_count <= 5:
                                self.logger.warning(f"âš ï¸ [vLLM SSE] JSON decode error on line {line_count}: {e}")
                            continue

                self.logger.debug(f"ğŸ“¤ [vLLM] Streamed {chunk_count} chunks (total lines: {line_count})")

    async def _stream_answer_ollama(self, prompt: str, question: str):
        """
        Stream answer using Ollama engine

        Args:
            prompt: The formatted prompt
            question: User's question (for logging)

        Yields:
            Text chunks as they're generated
        """
        messages = [{'role': 'user', 'content': prompt}]

        chunk_count = 0
        total_chunks = 0
        async for chunk in await self.ollama_client.chat(
            model=self.ollama_model,
            messages=messages,
            stream=True
        ):
            total_chunks += 1
            # Ollama returns ChatResponse object (not dict!)
            # For reasoning models, answer appears in 'content' after 'thinking' phase
            if hasattr(chunk, 'message') and chunk.message:
                # Stream only the final answer content, skip thinking tokens
                if chunk.message.content:
                    content = chunk.message.content
                    chunk_count += 1
                    yield content
                elif total_chunks <= 3:  # Log first 3 empty chunks
                    thinking_preview = chunk.message.thinking[:30] if chunk.message.thinking else ''
                    self.logger.debug(f"âš ï¸ Empty content chunk #{total_chunks}: thinking={thinking_preview}")

        self.logger.debug(f"ğŸ“¤ [Ollama] Streamed {chunk_count}/{total_chunks} chunks for question: {question[:50]}")

    def _build_answer_prompt(
        self,
        title: str,
        texts: str,
        history_user_query: str,
        question: str,
        toc: str
    ) -> str:
        """
        Build answer prompt (extracted for reuse in streaming and non-streaming)

        Returns:
            Formatted prompt string
        """
        return f"""**êµì¬ AI ì–´ì‹œìŠ¤í„´íŠ¸**

**ê·œì¹™:**
1. ğŸ‡°ğŸ‡· í•œêµ­ì–´ë§Œ (ì˜ì–´ ì ˆëŒ€ ê¸ˆì§€)
2. ì±… ì œëª© ê¸ˆì§€ â†’ "ì´ ì±…", "êµì¬" ì‚¬ìš©
3. **ë‹µë³€ ê¸¸ì´:** ì§ˆë¬¸ì— ë§ê²Œ ìœ ë™ì 

**ğŸ”¥ ë¬¸ì œ ë²ˆí˜¸ ì°¸ì¡° (CRITICAL):**
ì‚¬ìš©ìê°€ "ë¬¸ì œ 1ë²ˆ", "ë¬¸ì œ 2ë²ˆ", "1ë²ˆ ë¬¸ì œ" ë“±ì„ ì–¸ê¸‰í•˜ë©´:
â†’ ì•„ë˜ **ëŒ€í™” ê¸°ë¡**ì—ì„œ í•´ë‹¹ ë²ˆí˜¸ ì°¾ê¸° (ì˜ˆ: "**ë¬¸ì œ 1ë²ˆ**", "**ë¬¸ì œ 2ë²ˆ**")
â†’ í•´ë‹¹ ë¬¸ì œ ë‚´ìš© ë³µì‚¬
â†’ ê·¸ ë¬¸ì œì— ëŒ€í•œ ì •ë‹µ/í•´ì„¤ ì œê³µ
â†’ ë‹¤ë¥¸ ë¬¸ì œì™€ í˜¼ë™í•˜ì§€ ë§ ê²ƒ!

**ì¸ì‚¬:** "ì•ˆë…•" â†’ "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

---
**ëŒ€í™” ê¸°ë¡:**
{history_user_query[:1500] if history_user_query else "ì—†ìŒ"}

**ì±… ë‚´ìš©:**
{texts[:3000] if texts else "ì—†ìŒ"}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**
"""

    async def get_answer(
        self,
        title: str,
        texts: str,
        history_user_query: str,
        question: str,
        toc: str
    ) -> Dict[str, Any]:
        """
        Answer questions about book content (non-streaming version)

        Args:
            title: Book title
            texts: Related book content
            history_user_query: Previous conversation history
            question: User's question
            toc: Table of contents

        Returns:
            Response dict with answer text
        """
        prompt = self._build_answer_prompt(title, texts, history_user_query, question, toc)

        try:
            response = await self.generate_text(prompt, mode="chat")
            return response
        except Exception as e:
            self.logger.error(f"Error in get_answer: {e}")
            raise

    async def get_toc_answer(
        self,
        title: str,
        toc: str,
        question: str,
        show_full_details: bool = False,
        history_user_query: str = "[]"
    ) -> Dict[str, Any]:
        """
        Answer TOC-specific questions with simplified prompt (faster processing)

        This method is optimized for table of contents queries like:
        - "ëª©ì°¨ ë³´ì—¬ì¤˜" (show table of contents)
        - "2ì¥ ì œëª©ì´ ë­ì•¼?" (what's the title of chapter 2?)
        - "ëª‡ ì¥ê¹Œì§€ ìˆì–´?" (how many chapters are there?)

        Args:
            title: Book title
            toc: Table of contents (simple or full version)
            question: User's question about TOC
            show_full_details: If False, show only chapter-level; if True, show all details
            history_user_query: Previous conversation history (optional)

        Returns:
            Response dict with answer text
        """
        # Determine output format based on show_full_details flag
        if show_full_details:
            detail_instruction = "ëª©ì°¨ì˜ ëª¨ë“  ì¥ê³¼ ì ˆì„ í¬í•¨í•˜ì—¬ ìƒì„¸í•˜ê²Œ ë³´ì—¬ì£¼ì„¸ìš”."
        else:
            detail_instruction = "ëª©ì°¨ëŠ” **ì¥(Chapter) ìˆ˜ì¤€ë§Œ** ê°„ëµí•˜ê²Œ ë³´ì—¬ì£¼ì„¸ìš”. ì„¸ë¶€ ì ˆ(Section)ì€ ìƒëµí•˜ê³  ëŒ€ì œëª©ë§Œ í¬í•¨í•˜ì„¸ìš”."

        prompt = f"""ë‹¹ì‹ ì€ ëª©ì°¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì¤‘ìš” ê·œì¹™
**ì±… ì œëª©ì„ ì‘ë‹µì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
- "ì´ ì±…", "ë³¸ì„œ", "í•´ë‹¹ ë„ì„œ" ê°™ì€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì˜ˆì‹œ: âŒ "ì„±ê²©ì‹¬ë¦¬í•™ì˜ ëª©ì°¨" â†’ âœ… "ëª©ì°¨" ë˜ëŠ” "ì´ ì±…ì˜ ëª©ì°¨"

## ëª©ì°¨ ì •ë³´ (ì „ì²´)
{toc}

## ì´ì „ ëŒ€í™”
{history_user_query}

**ì¤‘ìš”**: í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì˜ í›„ì† ì§ˆë¬¸ì¸ ê²½ìš°, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
íŠ¹íˆ "ê°ê´€ì‹ìœ¼ë¡œ", "ì„œìˆ í˜•ìœ¼ë¡œ" ê°™ì€ í˜•ì‹ ë³€ê²½ ìš”ì²­ì€ ì´ì „ ë‚´ìš©ì„ í•´ë‹¹ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## ì‚¬ìš©ì ì§ˆë¬¸
{question}

## ë‹µë³€ ê°€ì´ë“œ
1. **ì±… ì œëª©ì„ ì§ì ‘ ì“°ì§€ ë§ˆì„¸ìš”** (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
2. ëª©ì°¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
3. ëª©ì°¨ ì „ì²´ë¥¼ ë³´ì—¬ë‹¬ë¼ëŠ” ìš”ì²­ì´ë©´:
   - {detail_instruction}
   - ë³´ê¸° ì¢‹ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì œê³µí•˜ì„¸ìš”
   - í˜ì´ì§€ ë²”ìœ„ë„ í•¨ê»˜ í‘œì‹œí•˜ì„¸ìš”
4. íŠ¹ì • ì •ë³´ë¥¼ ë¬»ëŠ”ë‹¤ë©´ (ì˜ˆ: "2ì¥ ì œëª©", "ëª‡ ì¥ê¹Œì§€"), í•´ë‹¹ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
5. ëª©ì°¨ì— ì—†ëŠ” ë‚´ìš©ì€ "ëª©ì°¨ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
6. ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

ë‹µë³€ (ìµœëŒ€ 1500ì):"""

        try:
            response = await self.generate_text(prompt, mode="chat")
            return response
        except Exception as e:
            self.logger.error(f"Error in get_toc_answer: {e}")
            raise

    async def get_summary(
        self,
        origin_text: str,
        book_title: str,
        section_title: str
    ) -> Dict[str, Any]:
        """
        Generate summary and extract keywords (replaces getSummary.js)

        Args:
            origin_text: Original text to summarize
            book_title: Book title
            section_title: Section title/TOC entry

        Returns:
            Dict with 'coreSummary' and 'sectionDetails'
        """
        prompt = f"""í•„ìš”í•œ í•˜ìœ„ ëª©ì°¨ ì‘ì„± ë° ìš”ì•½í•˜ê¸°.
ì£¼ì–´ì§„ ì‘ì—… ìˆœì„œëŒ€ë¡œ ì‘ì—…ì„ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

# 0. í˜„ì¬ ì²˜ë¦¬í•  ë¶€ë¶„ì— ëŒ€í•´ì„œ í™•ì¸í•˜ì„¸ìš”.
[ì±… ì œëª©]: {book_title}
[í˜„ì¬ ëª©ì°¨]: {section_title}

# 1. í˜„ì¬ ëª©ì°¨({section_title})ëŠ” ì–´ë–¤ ë²”ì£¼ì¸ê°€? (ë¶€,ì¥,ì ˆ,í•­)


# 2. coreSummary
ì´ ì±…ì—ì„œë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ ê°œë…ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë“¤ì„ , ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•œë‹¤.


# 3. í˜„ì¬ ëª©ì°¨ì˜ ë²”ì£¼ê°€ ì ˆì´ ì•„ë‹ë•Œ
ë‹¤ìŒì²˜ëŸ¼ ëª©ì°¨ì˜ ë²”ì£¼ê°€ ë¶€,ì¥,ê¸°íƒ€ ì¼ë•Œ
ì˜ˆ) 1ë¶€ ì´ì±…ì„ ì‹œì‘í•˜ë©´ì„œ
ì˜ˆ) ë“¤ì–´ê°€ë©´ì„œ
ì˜ˆ) 1ì¥ ê°œë¡ í•™
sectionDetails = [
      {{
        "title": "ì—†ìŒ",
        "pageStart": 0,
        "pageEnd": 0,
        "summary": "ì—†ìŒ"
      }},
]
**ì ˆëŒ€** í•˜ìœ„ ëª©ì°¨ë“¤ì„ ì‘ì„±í•˜ì§€ ì•ŠëŠ”ë‹¤. ë‹¨ì§€ 1ê°œë§Œ ìœ„ì˜ ê°’ëŒ€ë¡œ ì‘ì„±í•œë‹¤.



# 4. í˜„ì¬ ëª©ì°¨ê°€ ì ˆì¸ ê²½ìš°ì—
- 1. [ì²˜ë¦¬í•  ì›ë¬¸]ì€ í˜„ì¬ ëª©ì°¨ì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
- 2. [ì²˜ë¦¬í•  ì›ë¬¸]ì—ì„œ í˜„ì¬ ëª©ì°¨ì˜ í•˜ìœ„ ëª©ì°¨ë“¤ì„ ì°¾ì•„ì„œ ì •ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

"sectionDetails": [
      {{
        "title": "ì„¸ë¶€ì ˆ ì œëª©",
        "pageStart": ì‹œì‘í˜ì´ì§€(ì •ìˆ˜),
        "pageEnd": ëí˜ì´ì§€(ì •ìˆ˜),
        "summary": "100-200 ë‹¨ì–´ í•µì‹¬ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•œë‹¤."
      }},
      ...
    ]

# 5. ì°¸ê³ ë¬¸í—Œì€ ìš”ì•½í•˜ì§€ ì•ŠëŠ”ë‹¤. í‚¤ì›Œë“œ ì‘ì„±ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

# 6. ëª©ì°¨ëŠ” ìš”ì•½í•˜ì§€ ì•ŠëŠ”ë‹¤. í‚¤ì›Œë“œ ì‘ì„±ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

# 7. ìš©ì–´ ì„¤ëª…ì€ ìš”ì•½í•˜ì§€ ì•ŠëŠ”ë‹¤. í‚¤ì›Œë“œ ì‘ì„±ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

---



### [ì²˜ë¦¬í•  ì›ë¬¸]
{origin_text}
"""

        response_schema = {
            "type": "object",
            "properties": {
                "coreSummary": {
                    "type": "string",
                    "description": "ì´ ì±…ì—ì„œë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ ê°œë…ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë“¤ì„ , ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•œë‹¤."
                },
                "sectionDetails": {
                    "type": "array",
                    "description": "ëª©ì°¨ ì •ë³´ ëª©ë¡",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "pageStart": {"type": "integer"},
                            "pageEnd": {"type": "integer"},
                            "summary": {"type": "string"}
                        },
                        "required": ["title", "pageStart", "pageEnd", "summary"]
                    }
                }
            },
            "required": ["coreSummary", "sectionDetails"]
        }

        self.logger.info(f"ğŸ“„ ìš”ì•½ ìƒì„± ì¤‘... ì›ë¬¸ ê¸¸ì´: {len(origin_text)}ì")

        try:
            response = await self.generate_text(prompt, response_schema=response_schema, mode="chat")
            return response['result']
        except Exception as e:
            self.logger.error(f"Error in get_summary: {e}")
            raise

    async def get_image_to_text(
        self,
        image_data: Dict[str, str],
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """
        Extract text from PDF page image (replaces getImageToText.js)

        NOTE: This uses vision model for OCR. If vision model not available,
        this will need to use traditional OCR (tesseract, etc.)

        Args:
            image_data: Dict with 'data' (base64) and 'mimeType'
            max_retries: Number of retry attempts

        Returns:
            Dict with 'result' containing 'pageNumber' and 'extractedText'
        """
        prompt = """ë„ˆëŠ” ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ê°€ì•¼.
ì§€ê¸ˆë¶€í„° ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ ì¤˜. ê°€ì¥ ì¤‘ìš”í•œ ì›ì¹™ì€ **'ì‚¬ëŒì´ ì½ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ìˆœì„œ'**ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ëŠ” ê±°ì•¼.
**ì´ë¯¸ì§€ë¥¼ ë³„ë„ ë¶„ë¦¬í•˜ì§€ ì•Šê³ **, ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚˜ëŠ” ìœ„ì¹˜ì—
ì´ë¯¸ì§€ê°€ ë³´ì´ëŠ” ìœ„ì¹˜ì—ëŠ” ![â€¦](â€¦) ëŒ€ì‹ ,
[ì´ë¯¸ì§€: ì´ë¯¸ì§€ ì„¤ëª…] ì‹ìœ¼ë¡œ Alt-textë§Œ í…ìŠ¤íŠ¸ ì•ˆì— ì‚½ì…í•´ ì¤˜. í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì•¼ í•´.

ì´ë¥¼ ìœ„í•´ ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œì¤˜:

### 1. ì‹œê°ì  ë¶„ì„ ìš°ì„ : í…ìŠ¤íŠ¸ë¥¼ ì½ê¸° ì „ì—, ë¨¼ì € ë¬¸ì„œ ì „ì²´ì˜ ë ˆì´ì•„ì›ƒ(ë‹¨, ë¬¸ë‹¨, ê·¸ë¦¼ ë“±ì˜ ë°°ì¹˜)ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•´.


### 2. ì¸ê°„ì˜ ë…ì„œ íë¦„ ëª¨ë°©:
- ë§Œì•½ 1ë‹¨ êµ¬ì¡°ë¼ë©´, ìœ„ì—ì„œ ì•„ë˜ë¡œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•´.
- ë§Œì•½ ì‹ ë¬¸ì´ë‚˜ ë…¼ë¬¸ì²˜ëŸ¼ ì—¬ëŸ¬ ë‹¨ êµ¬ì¡°ë¼ë©´, ë°˜ë“œì‹œ ì™¼ìª½ ì²« ë²ˆì§¸ ë‹¨ì˜ ë‚´ìš©ì„ ìœ„ì—ì„œ ì•„ë˜ë¡œ ëª¨ë‘ ì¶”ì¶œí•œ ë’¤, ë‹¤ìŒ ë‹¨ìœ¼ë¡œ ë„˜ì–´ê°€ì„œ ì‘ì—…ì„ ë°˜ë³µí•´.
- ì œëª©ì´ë‚˜ ì´ˆë¡ì²˜ëŸ¼ í˜ì´ì§€ ì „ì²´ì— ê±¸ì³ ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” ê°€ì¥ ë¨¼ì € ì²˜ë¦¬í•´.

### 3. **ë³¸ë¬¸ ì¶”ì¶œ**
- ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ í‘œì˜ ë°ì´í„°, ì´ë¯¸ì§€ ì„¤ëª…ì„ ëª¨ë‘ í¬í•¨. ì´ë¯¸ì§€ì— ì •ë³´ê°€ ìˆë‹¤ë©´ ìš”ì•½í•˜ì§€ ë§ê³  ì „ì²´ë¥¼ ì‘ì„±í•œë‹¤.
- ì´ë¯¸ì§€ê°€ ë‚˜ì˜¤ë©´ [ì´ë¯¸ì§€: ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…] í˜•íƒœë¡œ **ë³¸ë¬¸ ì•ˆì—** ì‚½ì….
- ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì‘ì„±í•´ì•¼ í•œë‹¤. ê´€ê³„í‘œì‹œë“±ë„ ëª¨ë‘ ë§ˆì°¬ê°€ì§€ì´ë‹¤.


### 4. ë‚´ìš©ì˜ ì •í™•ì„±: í…ìŠ¤íŠ¸ê°€ ë’¤ì„ì´ê±°ë‚˜ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡, ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ ì •í™•í•œ ìˆœì„œì™€ ë‚´ìš©ìœ¼ë¡œ ë³€í™˜í•´ ì¤˜.

### 5. ì¶œë ¥ë‚´ìš©
- ì¶”ì¶œëœ í…ìŠ¤íŠ¸ : ì¼ë°˜ í…ìŠ¤íŠ¸ ë° í‘œì™€ ì´ë¯¸ì§€ë“± ëª¨ë“  ë‚´ìš©ì„ í…ìŠ¤íŠ¸í™” í•©ë‹ˆë‹¤. markdown ë¬¸ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì¶”ì¶œí•œ pdf ì˜ í˜ì´ì§€ ë²ˆí˜¸

### 6. í…ìŠ¤íŠ¸ê°€ ì—†ë‹¤ë©´ "ë‚´ìš©ì—†ìŒ" ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ!

### 7. ì´ë¯¸ì§€ë§Œ ìˆëŠ” í˜ì´ì§€ë©´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ì¶œë ¥í•˜ë©´ ë¨

### 8. ì €ì‘ê¶Œì— ê´€ë ¨ëœ ì •ë³´ë¥¼ ì œì™¸í•˜ê³  ì¶œë ¥í•´ì•¼ í•œë‹¤.

ì¶œë ¥ì€ json ì…ë‹ˆë‹¤.
"""

        response_schema = {
            "type": "object",
            "description": "PDFì˜ í•œ í˜ì´ì§€ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ì •ë³´",
            "properties": {
                "pageNumber": {
                    "type": "integer",
                    "description": "PDF ë‚´ í•´ë‹¹ í˜ì´ì§€ì˜ ì‹¤ì œ ë²ˆí˜¸"
                },
                "extractedText": {
                    "type": "string",
                    "description": "í•´ë‹¹ ì´ë¯¸ì§€(PDF í˜ì´ì§€)ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"
                }
            },
            "required": ["pageNumber", "extractedText"]
        }

        retries = 0
        while retries <= max_retries:
            try:
                response = await self.generate_text(
                    prompt,
                    response_schema=response_schema,
                    mode="vision",
                    image_data=image_data
                )
                self.logger.info("í…ìŠ¤íŠ¸ë³€í™˜ë¨")
                return response

            except Exception as err:
                retries += 1
                if retries > max_retries:
                    self.logger.error(f"âŒ Vision API {max_retries}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨: {err}")
                    return {
                        'result': {
                            'extractedText': '[Vision API 3íšŒ ì˜¤ë¥˜]'
                        }
                    }

                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, ì¬ì‹œë„ {retries}/{max_retries}...")
                import asyncio
                await asyncio.sleep(3)

    async def get_search_type(
        self,
        title: str,
        toc: str,
        history_user_query: str,
        query: str,
        previous_context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Determine search type and extract keywords (replaces getSearchType.js)

        Args:
            title: Book title
            toc: Table of contents (JSON string)
            history_user_query: Conversation history
            query: User query
            previous_context: Previous search context

        Returns:
            Dict with searchType, pages, coreKeywords, subKeywords, reason
        """
        # ğŸ”¥ Smart TOC filtering for different query types
        import re

        # 1. Detect conversation-reference queries (don't need TOC)
        # Patterns: "ë¬¸ì œ 1ë²ˆ", "1ë²ˆ ë¬¸ì œ", "ì •ë‹µ", "í•´ì„¤", "ê·¸ê±°", "ë°©ê¸ˆ", etc.
        conversation_ref_patterns = [
            r'\d+ë²ˆ\s*(ë¬¸ì œ|ì •ë‹µ|í•´ì„¤)',  # "1ë²ˆ ë¬¸ì œ", "2ë²ˆ ì •ë‹µ"
            r'(ë¬¸ì œ|ì •ë‹µ|í•´ì„¤)\s*\d+ë²ˆ',  # "ë¬¸ì œ 1ë²ˆ", "ì •ë‹µ 2ë²ˆ"
            r'^(ì •ë‹µ|í•´ì„¤|í’€ì´)',  # "ì •ë‹µ", "í•´ì„¤" (ë¬¸ì¥ ì‹œì‘)
            r'(ê·¸ê±°|ì €ê±°|ë°©ê¸ˆ|ìœ„|ì•„ë˜|ì´ì „)',  # ëŒ€í™” ì°¸ì¡°
        ]

        is_conversation_ref = any(re.search(pattern, query, re.IGNORECASE) for pattern in conversation_ref_patterns)

        if is_conversation_ref:
            toc = ""  # Remove TOC for conversation-reference queries
            self.logger.info(f"ğŸ—£ï¸ Conversation-reference query detected, TOC removed to save tokens")
        else:
            # 2. Detect chapter-specific queries and filter TOC
            chapter_pattern = r'(\d+)(?:-(\d+))?ì¥'
            chapter_match = re.search(chapter_pattern, query)

            if chapter_match and toc:
                start_chapter = int(chapter_match.group(1))
                end_chapter = int(chapter_match.group(2)) if chapter_match.group(2) else start_chapter

                # Parse TOC JSON and filter for requested chapters
                try:
                    toc_lines = toc.strip().split('\n')
                    filtered_toc_lines = []
                    for line in toc_lines:
                        # Check if line contains any of the requested chapters
                        for ch in range(start_chapter, end_chapter + 1):
                            if f"{ch}ì¥" in line:
                                filtered_toc_lines.append(line)
                                break

                    if filtered_toc_lines:
                        toc = '\n'.join(filtered_toc_lines)
                        self.logger.info(f"ğŸ¯ Filtered TOC to chapters {start_chapter}-{end_chapter} ({len(filtered_toc_lines)} entries, {len(toc)} chars)")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Failed to filter TOC by chapters: {e}")

        prompt = f"""**ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ íƒ€ì… ê²°ì •**

ì§ˆë¬¸: "{query}"

ëŒ€í™”: {history_user_query[:300] if history_user_query else 'ì—†ìŒ'}
ì´ì „: {json.dumps(previous_context, ensure_ascii=False) if previous_context else 'ì—†ìŒ'}

**ê²€ìƒ‰ íƒ€ì…:**
1. `toc`: ëª©ì°¨ íƒìƒ‰
2. `page`: íŠ¹ì • í˜ì´ì§€
3. `summary`: ì±•í„° ìš”ì•½ (ì˜ˆ: "1ì¥ ìš”ì•½" â†’ ëª©ì°¨ì—ì„œ 1ì¥ í˜ì´ì§€ ì°¾ì•„ `pages` ì§€ì •!)
4. `keyword`: í‚¤ì›Œë“œ ê²€ìƒ‰
5. `semantic`: ì˜ë¯¸ ê²€ìƒ‰
6. `quiz`: ë¬¸ì œ ì¶œì œ
7. `followup`: ì¸ì‚¬/ì¼ìƒ/ì´ì „ ëŒ€í™” ì°¸ì¡°
8. `irrelevant`: êµì¬ ì™„ì „ ë¬´ê´€ (**ê·¹íˆ ë“œë¬¼ê²Œ, í™•ì‹¤í•  ë•Œë§Œ!**)
   - ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨", "ìš”ë¦¬ ë°©ë²•", "ì˜í™” ì¶”ì²œ"
   - âš ï¸ ì‹¬ë¦¬í•™/ì„±ê²© ê´€ë ¨ ìš©ì–´ëŠ” ì ˆëŒ€ irrelevant ì•„ë‹˜!
   - ì˜ˆ: MBTI, ì„±ê²©ê²€ì‚¬, ì‹¬ë¦¬ì´ë¡  â†’ keyword/semantic ê²€ìƒ‰!

**ğŸ”¥ ì¤‘ìš”:**
- "1ì¥ ìš”ì•½", "2ì¥ ë‚´ìš©" â†’ `summary` + ëª©ì°¨ì—ì„œ í˜ì´ì§€ ì°¾ì•„ `pages` ì§€ì •!
- **ë¹„êµ/ì°¨ì´ ì§ˆë¬¸ â†’ `keyword` ê²€ìƒ‰!**
  - "Aì™€ Bì˜ ì°¨ì´", "A vs B", "Aì™€ B ë¹„êµ" â†’ `keyword`
  - ì˜ˆ: "ë‚´í–¥í˜•ê³¼ ì™¸í–¥í˜•ì˜ ì°¨ì´ëŠ”?" â†’ `keyword` (summary ì•„ë‹˜!)
- ì¸ì‚¬("ì•ˆë…•") â†’ `followup`
- **í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ìƒì„±!** (í•œêµ­ì–´ ì±… â†’ í•œêµ­ì–´ í‚¤ì›Œë“œ)
  - ì˜ˆ: "ë‚´í–¥í˜•" (O), "introvert" (X)
  - ì˜ˆ: "ìê¸°íš¨ëŠ¥ê°" (O), "self-efficacy" (X)

**JSON ì¶œë ¥:**
{{
  "searchType": "summary",
  "pages": "18-74",
  "coreKeywords": [],
  "subKeywords": [],
  "usePreviousContext": false,
  "reason": "1ì¥ ìš”ì•½ ìš”ì²­"
}}

---
**ì±…:** {title}
**ëª©ì°¨:**
{toc}

**ì§ˆë¬¸:** {query}
"""

        # ğŸ”¥ Dynamic TOC truncation to prevent token overflow
        # Estimate tokens: Korean text ~0.7 chars/token
        # Balance: ëŒ€í˜• TOC ì§€ì› vs vLLM í† í° ì œí•œ (8K context)
        max_toc_tokens = 6000  # Reserve tokens for TOC (optimized for ~200 entries)
        estimated_toc_tokens = int(len(toc) * 0.7)

        if estimated_toc_tokens > max_toc_tokens:
            max_toc_chars = int(max_toc_tokens / 0.7)
            toc_truncated = toc[:max_toc_chars]
            self.logger.warning(f"âš ï¸ TOC too long ({estimated_toc_tokens} tokens), truncating to {max_toc_tokens} tokens ({max_toc_chars} chars)")

            # Re-build prompt with truncated TOC
            prompt = f"""**ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ íƒ€ì… ê²°ì •**

ì§ˆë¬¸: "{query}"

ëŒ€í™”: {history_user_query[:300] if history_user_query else 'ì—†ìŒ'}
ì´ì „: {json.dumps(previous_context, ensure_ascii=False) if previous_context else 'ì—†ìŒ'}

**ê²€ìƒ‰ íƒ€ì…:**
1. `toc`: ëª©ì°¨ íƒìƒ‰
2. `page`: íŠ¹ì • í˜ì´ì§€
3. `summary`: ì±•í„° ìš”ì•½ (ì˜ˆ: "1ì¥ ìš”ì•½" â†’ ëª©ì°¨ì—ì„œ 1ì¥ í˜ì´ì§€ ì°¾ì•„ `pages` ì§€ì •!)
4. `keyword`: í‚¤ì›Œë“œ ê²€ìƒ‰
5. `semantic`: ì˜ë¯¸ ê²€ìƒ‰
6. `quiz`: ë¬¸ì œ ì¶œì œ
7. `followup`: ì¸ì‚¬/ì¼ìƒ/ì´ì „ ëŒ€í™” ì°¸ì¡°
8. `irrelevant`: êµì¬ ì™„ì „ ë¬´ê´€ (**ê·¹íˆ ë“œë¬¼ê²Œ, í™•ì‹¤í•  ë•Œë§Œ!**)
   - ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨", "ìš”ë¦¬ ë°©ë²•", "ì˜í™” ì¶”ì²œ"
   - âš ï¸ ì‹¬ë¦¬í•™/ì„±ê²© ê´€ë ¨ ìš©ì–´ëŠ” ì ˆëŒ€ irrelevant ì•„ë‹˜!
   - ì˜ˆ: MBTI, ì„±ê²©ê²€ì‚¬, ì‹¬ë¦¬ì´ë¡  â†’ keyword/semantic ê²€ìƒ‰!

**ğŸ”¥ ì¤‘ìš”:**
- "1ì¥ ìš”ì•½", "2ì¥ ë‚´ìš©" â†’ `summary` + ëª©ì°¨ì—ì„œ í˜ì´ì§€ ì°¾ì•„ `pages` ì§€ì •!
- **ë¹„êµ/ì°¨ì´ ì§ˆë¬¸ â†’ `keyword` ê²€ìƒ‰!**
  - "Aì™€ Bì˜ ì°¨ì´", "A vs B", "Aì™€ B ë¹„êµ" â†’ `keyword`
  - ì˜ˆ: "ë‚´í–¥í˜•ê³¼ ì™¸í–¥í˜•ì˜ ì°¨ì´ëŠ”?" â†’ `keyword` (summary ì•„ë‹˜!)
- ì¸ì‚¬("ì•ˆë…•") â†’ `followup`
- **í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ìƒì„±!** (í•œêµ­ì–´ ì±… â†’ í•œêµ­ì–´ í‚¤ì›Œë“œ)
  - ì˜ˆ: "ë‚´í–¥í˜•" (O), "introvert" (X)
  - ì˜ˆ: "ìê¸°íš¨ëŠ¥ê°" (O), "self-efficacy" (X)

**JSON ì¶œë ¥:**
{{
  "searchType": "summary",
  "pages": "18-74",
  "coreKeywords": [],
  "subKeywords": [],
  "usePreviousContext": false,
  "reason": "1ì¥ ìš”ì•½ ìš”ì²­"
}}

---
**ì±…:** {title}
**ëª©ì°¨ (ì¼ë¶€, ì „ì²´ {len(toc)} ë¬¸ì ì¤‘ {max_toc_chars} ë¬¸ì):**
{toc_truncated}

**ì§ˆë¬¸:** {query}
"""

        response_schema = {
            "type": "object",
            "properties": {
                "usePreviousContext": {
                    "type": "boolean",
                    "description": "ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¬ì‚¬ìš©í• ì§€ ì—¬ë¶€"
                },
                "searchType": {
                    "type": "string",
                    "enum": ["page", "keyword", "summary", "toc", "irrelevant", "followup", "quiz", "semantic"],
                    "description": "ê²€ìƒ‰ì— ì‚¬ìš©í•  ë°©ë²•"
                },
                "pages": {
                    "type": "string",
                    "description": "ì •ë³´ê°€ í•„ìš”í•œ í˜ì´ì§€ ë²”ìœ„"
                },
                "coreKeywords": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "keyword": {"type": "string"},
                            "alternatives": {"type": "array", "items": {"type": "string"}}
                        }
                    }
                },
                "subKeywords": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "keyword": {"type": "string"},
                            "alternatives": {"type": "array", "items": {"type": "string"}}
                        }
                    }
                },
                "reason": {
                    "type": "string",
                    "description": "ê²€ìƒ‰íƒ€ì…ì„ ê²°ì •í•œ ì´ìœ "
                }
            },
            "required": ["usePreviousContext", "searchType", "pages", "reason"]
        }

        try:
            response = await self.generate_text(prompt, response_schema=response_schema, mode="chat")
            return response
        except Exception as e:
            self.logger.error(f"Error in get_search_type: {e}")
            raise

    async def get_format_toc(self, manual_toc: str) -> str:
        """
        Format table of contents (replaces getFormatToc.js)

        Args:
            manual_toc: Raw TOC text

        Returns:
            Formatted TOC string
        """
        prompt = f"""
[ì…ë ¥ëœ ëª©ì°¨ë°ì´í„°]ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ 'ëª©ì°¨|í˜ì´ì§€ë²ˆí˜¸' í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ë‹¤ì‹œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì¶œë ¥ ì—†ì´, ì˜¤ì§ 'ëª©ì°¨|í˜ì´ì§€ë²ˆí˜¸' í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ëª©ì°¨ ì •ë¦¬ ê·œì¹™:


1. ëª©ì°¨ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì„œ, ë¶€, ì¥, ì ˆì˜ êµ¬ë¶„ì„ í™•ì¸í•˜ì„¸ìš”. í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œê°€ ì•„ë‹ˆë¼ ì˜ë¯¸ì ìœ¼ë¡œ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.
ì±…ë§ˆë‹¤ ë¶€,ì¥,ì ˆì˜ í‘œì‹œí•˜ëŠ” ë°©ë²•ì´ ë‹¤ë¥´ë©°, ë¶€,ì¥,ì ˆì˜ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤.

2. ì…ë ¥ëœ ëª©ì°¨ë°ì´í„°ì˜ ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì±…ìœ¼ë¡œ ì§„í–‰ë˜ë¯€ë¡œ, ê° ì±…ì˜ ë¶€,ì¥,ì ˆì˜ ê·œì¹™ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. ë¶€ì™€ ì¥ì€ 1ë¶€, 1ì¥ ì´ë¼ê³  í†µì¼í•´ì„œ ì‘ì„±í•©ë‹ˆë‹¤.
ì˜ˆ) 1ë¶€ ë¶€ì˜ì œëª©í…ìŠ¤íŠ¸
ì˜ˆ) 1ì¥ ì¥ì˜ì œëª©í…ŒìŠ¤íŠ¸

4. ì ˆì˜ ê²½ìš°ì—ëŠ” í•´ë‹¹ ì ˆì´ ì†í•˜ëŠ” ì¥ì˜ ë²ˆí˜¸ë¥¼ ì´ìš©í•´ì„œ ìˆœì„œëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
ì˜ˆ) 1ì¥ì— í¬í•¨ë˜ì—ˆë‹¤ë©´, 1-1 ì ˆì˜ì œëª©

5. ì¥ì´ë‚˜ ë¶€ ì´ì „ì— ë‚˜ì˜¤ëŠ” ì„œë¬¸, ì†Œê°œ ë“± ë…ë¦½ì ì¸ ë‚´ìš©ì—ëŠ” ì¥ì´ë‚˜ ì ˆ í‘œì‹œë¥¼ í•˜ì§€ ì•Šê³  ì œëª©ë§Œ ê¸°ì…í•©ë‹ˆë‹¤.
ì˜ˆì‹œ: ì˜®ê¸´ì´ ì„œë¬¸|5



### [ì…ë ¥ëœ ëª©ì°¨ë°ì´í„°]
{manual_toc}
"""

        try:
            response = await self.generate_text(prompt, mode="chat")
            return response['result']
        except Exception as e:
            self.logger.error(f"Error in get_format_toc: {e}")
            raise

    async def extract_keywords(
        self,
        text: str,
        model: Optional[str] = None,
        num_keywords: int = 5
    ) -> List[str]:
        """
        Extract keywords from text (used by Chat API)

        This is a wrapper around the generate_text method,
        similar to the original extract_keywords in ollama_service.

        Args:
            text: Input text
            model: Model to use
            num_keywords: Number of keywords to extract

        Returns:
            List of keywords
        """
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {num_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í‚¤ì›Œë“œ:"""

        try:
            model_name = model or self.model
            response = await self.client.generate(
                model=model_name,
                prompt=prompt,
                keep_alive=-1  # Keep model in memory indefinitely
            )

            # Parse keywords
            keywords_text = response['response'].strip()
            keywords = [k.strip() for k in keywords_text.split(',')]

            return keywords[:num_keywords]

        except Exception as e:
            self.logger.error(f"Error extracting keywords: {e}")
            raise

    async def simple_chat(
        self,
        message: str,
        context: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None,
        model: Optional[str] = None
    ) -> str:
        """
        Simple chat function (used by Chat API /chat endpoint)

        This is simpler than get_answer() - no book-specific prompts.
        Just regular conversation with optional context.

        Args:
            message: User message
            context: Optional context (e.g., PDF content)
            conversation_history: Previous messages
            model: Model to use

        Returns:
            AI response text
        """
        try:
            model_name = model or self.model

            # Build messages
            messages = []

            # Add conversation history
            if conversation_history:
                messages.extend(conversation_history)

            # Build user message with context
            user_content = message
            if context:
                user_content = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {message}"""

            messages.append({
                'role': 'user',
                'content': user_content
            })

            # Generate response using Ollama client chat API
            from ollama import AsyncClient
            client = AsyncClient(host=settings.OLLAMA_BASE_URL)

            response = await client.chat(
                model=model_name,
                messages=messages
            )

            return response['message']['content']

        except Exception as e:
            self.logger.error(f"Error in simple_chat: {e}")
            raise


# Create singleton instance
llm_service = LLMService()
