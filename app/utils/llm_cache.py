"""
LLM Response Cache
LLM ì‘ë‹µì„ ìºì‹±í•˜ì—¬ ë°˜ë³µ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ
"""
import hashlib
import time
import logging
from typing import Optional, Dict, Any
from collections import OrderedDict


class LLMCache:
    """
    LLM ì‘ë‹µ ìºì‹œ

    Features:
    - LRU eviction policy
    - TTL (Time To Live)
    - Query + Context ê¸°ë°˜ ìºì‹±
    """

    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):
        """
        Args:
            max_size: ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜
            ttl_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„)
        """
        self.cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()
        self.max_size = max_size
        self.ttl_seconds = ttl_hours * 3600
        self.logger = logging.getLogger(__name__)

        # Statistics
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'expirations': 0
        }

    def _get_cache_key(self, query: str, context: str = "") -> str:
        """
        ìºì‹œ í‚¤ ìƒì„± (ì¿¼ë¦¬ + ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ)

        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            context: ì»¨í…ìŠ¤íŠ¸ (ISBN + ê²€ìƒ‰ ìœ í˜•, ì‹¤ì œ ë‚´ìš© ì œì™¸)

        Returns:
            MD5 í•´ì‹œ í‚¤
        """
        # Normalize query (ì†Œë¬¸ì, ê³µë°± ì •ë¦¬, êµ¬ë‘ì  ì œê±°)
        normalized_query = query.lower().strip()

        # ì§ˆë¬¸ ì •ê·œí™”: êµ¬ë‘ì , ì¡°ì‚¬ ì œê±°, ê³µë°± ì •ë¦¬
        import re
        # 1. êµ¬ë‘ì  ì œê±°
        normalized_query = re.sub(r'[?!.,\s]+', ' ', normalized_query).strip()

        # 2. í•œê¸€ ì¡°ì‚¬ ì œê±° (ì€/ëŠ”/ì´/ê°€/ì„/ë¥¼/ì™€/ê³¼/ì˜/ì—/ì—ì„œ/ìœ¼ë¡œ/ë¡œ)
        # ë‹¨ì–´ì— ë¶™ì–´ìˆëŠ” ì¡°ì‚¬ë„ ì œê±° (ì˜ˆ: "ë‚´í–¥í˜•ê³¼" -> "ë‚´í–¥í˜•")
        particles = [
            'ì€ëŠ”', 'ì´ê°€', 'ì„ë¥¼', 'ì™€ê³¼',  # ë³µí•© ì¡°ì‚¬
            'ì—ì„œ', 'ìœ¼ë¡œ',  # 2ê¸€ì ì¡°ì‚¬
            'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì™€', 'ê³¼', 'ì˜', 'ì—', 'ë¡œ'  # 1ê¸€ì ì¡°ì‚¬
        ]
        for particle in particles:
            # í•œê¸€ ë’¤ì— ë¶™ì€ ì¡°ì‚¬ ì œê±° (ì˜ˆ: "ë‚´í–¥í˜•ê³¼" -> "ë‚´í–¥í˜• ")
            normalized_query = re.sub(rf'([ê°€-í£]){particle}(\s|$)', r'\1 ', normalized_query)
            # ê³µë°± ì‚¬ì´ì˜ ì¡°ì‚¬ ì œê±°
            normalized_query = re.sub(rf'\s+{particle}\s+', ' ', normalized_query)

        # 3. ê³µë°± ì •ë¦¬
        normalized_query = re.sub(r'\s+', ' ', normalized_query).strip()

        # 4. ë™ì˜ì–´ í†µì¼
        synonyms = {
            'ë¬´ì—‡': 'ë­',
            'ì°¨ì´ì ': 'ì°¨ì´',
            'ì°¨ì´ëŠ”': 'ì°¨ì´',
            'ë‹¤ë¥¸ì ': 'ì°¨ì´',
            'êµ¬ë³„': 'ì°¨ì´'
        }
        for original, replacement in synonyms.items():
            normalized_query = normalized_query.replace(original, replacement)

        # ContextëŠ” ISBN + search_typeë§Œ í¬í•¨ (ì‹¤ì œ ê²€ìƒ‰ ë‚´ìš© ì œì™¸)
        # ì´ë ‡ê²Œ í•˜ë©´ ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” í•­ìƒ ê°™ì€ ìºì‹œ í‚¤ê°€ ìƒì„±ë¨
        normalized_context = context.lower().strip()

        # Create hash key
        text = f"{normalized_query}|||{normalized_context}"
        hash_key = hashlib.md5(text.encode('utf-8')).hexdigest()

        # Debug logging
        self.logger.debug(f"ğŸ”‘ Cache key generation:")
        self.logger.debug(f"   Original: '{query}'")
        self.logger.debug(f"   Normalized: '{normalized_query}'")
        self.logger.debug(f"   Context: '{context}' -> '{normalized_context}'")
        self.logger.debug(f"   Hash: {hash_key}")

        return hash_key

    def get(self, query: str, context: str = "") -> Optional[Dict[str, Any]]:
        """
        ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ

        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            context: ì»¨í…ìŠ¤íŠ¸

        Returns:
            ìºì‹œëœ ì‘ë‹µ ë˜ëŠ” None
        """
        key = self._get_cache_key(query, context)

        if key not in self.cache:
            self.stats['misses'] += 1
            return None

        # Get entry
        entry = self.cache[key]

        # Check TTL
        age = time.time() - entry['timestamp']
        if age > self.ttl_seconds:
            # Expired
            del self.cache[key]
            self.stats['expirations'] += 1
            self.stats['misses'] += 1
            self.logger.info(f"Cache expired (age: {age:.1f}s): {query[:50]}")
            return None

        # Move to end (LRU)
        self.cache.move_to_end(key)

        self.stats['hits'] += 1
        self.logger.info(f"âœ… Cache hit (age: {age:.1f}s): {query[:50]}")

        return entry['response']

    def set(self, query: str, context: str, response: Dict[str, Any]) -> None:
        """
        ìºì‹œì— ì‘ë‹µ ì €ì¥

        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            context: ì»¨í…ìŠ¤íŠ¸
            response: LLM ì‘ë‹µ
        """
        key = self._get_cache_key(query, context)

        # Check if already exists (update)
        if key in self.cache:
            del self.cache[key]

        # Check size limit
        if len(self.cache) >= self.max_size:
            # Remove oldest (first item)
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            self.stats['evictions'] += 1
            self.logger.info(f"Cache eviction (size: {self.max_size})")

        # Add new entry
        self.cache[key] = {
            'response': response,
            'timestamp': time.time(),
            'query': query[:100],  # For debugging
            'context': context[:100]
        }

        self.logger.info(f"ğŸ’¾ Cache set: {query[:50]} (total: {len(self.cache)})")

    def clear(self) -> None:
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        count = len(self.cache)
        self.cache.clear()
        self.logger.info(f"Cache cleared ({count} entries)")

    def get_stats(self) -> Dict[str, Any]:
        """
        ìºì‹œ í†µê³„ ì¡°íšŒ

        Returns:
            í†µê³„ ì •ë³´
        """
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_rate = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0

        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'hit_rate': f"{hit_rate:.1f}%",
            'evictions': self.stats['evictions'],
            'expirations': self.stats['expirations'],
            'ttl_hours': self.ttl_seconds / 3600
        }

    def cleanup_expired(self) -> int:
        """
        ë§Œë£Œëœ í•­ëª© ì •ë¦¬

        Returns:
            ì‚­ì œëœ í•­ëª© ìˆ˜
        """
        now = time.time()
        expired_keys = []

        for key, entry in self.cache.items():
            age = now - entry['timestamp']
            if age > self.ttl_seconds:
                expired_keys.append(key)

        for key in expired_keys:
            del self.cache[key]

        if expired_keys:
            self.logger.info(f"Cleaned up {len(expired_keys)} expired entries")

        return len(expired_keys)


# Global instance
_llm_cache = None


def get_llm_cache() -> LLMCache:
    """
    ê¸€ë¡œë²Œ LLM ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ

    Returns:
        LLMCache ì¸ìŠ¤í„´ìŠ¤
    """
    global _llm_cache
    if _llm_cache is None:
        # ë” ê³µê²©ì ì¸ ìºì‹±: í¬ê¸° 3ë°°, TTL 3ì¼
        _llm_cache = LLMCache(max_size=3000, ttl_hours=72)
    return _llm_cache


# Singleton instance for direct import
llm_cache = get_llm_cache()
